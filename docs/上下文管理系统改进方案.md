# ä¸Šä¸‹æ–‡ç®¡ç†ç³»ç»Ÿæ”¹è¿›æ–¹æ¡ˆ

## æ¦‚è¿°

åŸºäºå¯¹ç°æœ‰ä¼ä¸šçº§Agentç³»ç»Ÿä¸Šä¸‹æ–‡ç®¡ç†æ¶æ„çš„æ·±åº¦åˆ†æï¼Œå½“å‰ç³»ç»Ÿå·²å…·å¤‡å®Œå–„çš„ä¸‰å±‚æ¶æ„è®¾è®¡ï¼ˆContextå±‚ã€Sessionå±‚ã€Memoryå±‚ï¼‰å’Œä¼ä¸šçº§ç‰¹æ€§ã€‚æœ¬æ–¹æ¡ˆé’ˆå¯¹å¤§è§„æ¨¡éƒ¨ç½²å’Œå¤æ‚åœºæ™¯éœ€æ±‚ï¼Œæå‡ºå››å¤§æ ¸å¿ƒæ”¹è¿›æ–¹å‘ï¼Œä»¥è¿›ä¸€æ­¥æå‡ç³»ç»Ÿçš„æ€§èƒ½ã€å¯æ‰©å±•æ€§å’Œæ™ºèƒ½åŒ–æ°´å¹³ã€‚

## ç°æœ‰æ¶æ„æ·±åº¦åˆ†æ

### å½“å‰ç³»ç»Ÿæ¶æ„ç‰¹ç‚¹
åŸºäºå®é™…ä»£ç åˆ†æï¼Œç°æœ‰çŠ¶æ€åŸŸå…·å¤‡ä»¥ä¸‹å®Œå–„çš„æ¶æ„ï¼š

#### 1. ä¸‰å±‚çŠ¶æ€ç®¡ç†æ¶æ„
- **Contextå±‚** (`src/state/context.py`)ï¼šæ¶ˆæ¯å†å²ã€ä¸Šä¸‹æ–‡ç±»å‹ã€trace_idç»‘å®š
- **Sessionå±‚** (`src/state/context/session.py`)ï¼šä¼šè¯ç”Ÿå‘½å‘¨æœŸã€å¿«ç…§æœºåˆ¶ã€çŠ¶æ€æœºç®¡ç†
- **Memoryå±‚** (`src/state/memory.py` + `memory_enhanced.py`)ï¼šå†…å­˜æ¡ç›®ã€åˆ†åŒºç´¢å¼•ã€æŒä¹…åŒ–

#### 2. ä¼ä¸šçº§ç‰¹æ€§å®ç°
- **å¤šç§Ÿæˆ·æ”¯æŒ**ï¼šSessionæ”¯æŒtenant_idéš”ç¦»
- **è¿½è¸ªé“¾è·¯**ï¼šå®Œæ•´çš„trace_id + context_idå…³è”
- **æƒé™æ§åˆ¶**ï¼šåŸºäºagent_idçš„è®¿é—®æ§åˆ¶
- **ç›‘æ§ç»Ÿè®¡**ï¼šå®Œæ•´çš„statsç»Ÿè®¡å’Œå¥åº·æ£€æŸ¥

#### 3. ç°æœ‰æŠ€æœ¯ä¼˜åŠ¿
- **åˆ†åŒºç´¢å¼•**ï¼šcontext_partitionsã€trace_partitionsé«˜æ•ˆæ£€ç´¢
- **å¼‚æ­¥æ“ä½œ**ï¼šå…¨å¼‚æ­¥APIè®¾è®¡ï¼Œæ”¯æŒé«˜å¹¶å‘
- **çŠ¶æ€æœºç®¡ç†**ï¼šSessionStatuså®Œæ•´çš„çŠ¶æ€è½¬æ¢
- **å¿«ç…§æ¢å¤**ï¼šSessionSnapshotæ”¯æŒçŠ¶æ€å›æ»š
- **å†…å­˜æ¨¡æ¿**ï¼šMemoryEntryç»“æ„åŒ–å­˜å‚¨

### æ¶æ„å®Œå–„åº¦è¯„ä¼°
- âœ… **åŸºç¡€åŠŸèƒ½**ï¼šæ¶ˆæ¯å†å²ã€ä¼šè¯ç®¡ç†ã€å†…å­˜å­˜å‚¨ï¼ˆå®Œå¤‡ï¼‰
- âœ… **ä¼ä¸šç‰¹æ€§**ï¼šå¤šç§Ÿæˆ·ã€æƒé™ã€ç›‘æ§ã€è¿½è¸ªï¼ˆå®Œå¤‡ï¼‰
- âœ… **æ€§èƒ½ä¼˜åŒ–**ï¼šåˆ†åŒºç´¢å¼•ã€å¼‚æ­¥æ“ä½œã€ç¼“å­˜æœºåˆ¶ï¼ˆå®Œå¤‡ï¼‰
- âœ… **å¯é æ€§**ï¼šå¥åº·æ£€æŸ¥ã€å¿«ç…§æ¢å¤ã€çŠ¶æ€æœºï¼ˆå®Œå¤‡ï¼‰
- ğŸ”„ **æ™ºèƒ½åŒ–**ï¼šè¯­ä¹‰æ£€ç´¢ã€æ™ºèƒ½å‹ç¼©ï¼ˆå¾…å¢å¼ºï¼‰
- ğŸ”„ **åˆ†å¸ƒå¼**ï¼šè·¨èŠ‚ç‚¹åŒæ­¥ã€å¤šAgentåä½œï¼ˆå¾…å¢å¼ºï¼‰

### ç°æœ‰ä»£ç ä¼˜åŠ¿
1. **å®Œæ•´çš„æ•°æ®ç»“æ„**ï¼šMemoryEntryã€SessionMetaã€SessionSnapshot
2. **æˆç†Ÿçš„åˆ†åŒºæœºåˆ¶**ï¼šåŸºäºcontext_idå’Œtrace_idçš„é«˜æ•ˆç´¢å¼•
3. **å¥å…¨çš„ç”Ÿå‘½å‘¨æœŸ**ï¼šåˆå§‹åŒ–ã€å¥åº·æ£€æŸ¥ã€æ¸…ç†æœºåˆ¶
4. **ç»Ÿä¸€çš„æ¥å£è®¾è®¡**ï¼šå¼‚æ­¥APIã€é”™è¯¯å¤„ç†ã€æ—¥å¿—è®°å½•

## ä¼ä¸šçº§çŠ¶æ€ç®¡ç†ä½“ç³»è®¾è®¡åŸåˆ™

### çŠ¶æ€ç³»ç»Ÿåˆ†å±‚æ¶æ„
åŸºäºä¼ä¸šçº§Agentç³»ç»Ÿçš„å®é™…éœ€æ±‚ï¼ŒçŠ¶æ€ç³»ç»Ÿåº”æ‰¿æ‹…ä»¥ä¸‹æ ¸å¿ƒèŒè´£ï¼š

| åŠŸèƒ½ç»´åº¦ | æè¿° | å¯¹åº”æ¨¡å— |
|---------|------|---------|
| **è®°ä½è¿‡å»** | ä¿å­˜ä»»åŠ¡è¾“å…¥/Agentå†³ç­–/å·¥å…·ç»“æœ | Context + Memory |
| **æ„ŸçŸ¥å½“å‰** | æ„é€ å½“å‰ä¸Šä¸‹æ–‡ï¼šç”¨æˆ·æ„å›¾+å½“å‰è½®è¾“å…¥+ä¼šè¯å†å² | Session |
| **æŒ‡å¯¼æœªæ¥** | ä¸ºReasoneræä¾›ç­–ç•¥ä¾æ®ï¼šè®°å¿†å›è°ƒã€è¡Œä¸ºè°ƒæ•´ | LongTerm Memory |
| **è¿½æº¯é—®é¢˜** | æ”¯æŒTraceå›æ”¾/é”™è¯¯è°ƒè¯•/çŠ¶æ€é‡æ”¾/å¤±è´¥æ¢å¤ | TraceWriter |

### çŠ¶æ€ä½œç”¨åŸŸä¸‰å±‚ç»“æ„
| å±‚çº§ | è¯´æ˜ | ç°æœ‰æ¨¡å—æ˜ å°„ |
|------|------|-------------|
| **Agentå†…éƒ¨** | å½“å‰Agentçš„å±€éƒ¨ä¸Šä¸‹æ–‡çŠ¶æ€ | `Context` (short_term) |
| **Agentä¹‹é—´** | Sessionçº§åˆ«ä»»åŠ¡çŠ¶æ€ååŒ | `Session` (multi-step) |
| **ç³»ç»Ÿå…¨å±€** | å¤šä»»åŠ¡ã€å¤šAgentã€è·¨å¤©çŠ¶æ€ç®¡ç† | `Memory` (long_term) + `TraceWriter` |

### æ¨èçŠ¶æ€æ€»çº¿æ¥å£ç»“æ„
```python
class AgentState:
    """ç»Ÿä¸€çš„çŠ¶æ€è®¿é—®æ¥å£"""
    def __init__(self, task_id: str):
        self.context = load_context(task_id)        # çŸ­æœŸä¸Šä¸‹æ–‡
        self.session = load_session(task_id)        # ä¼šè¯çŠ¶æ€
        self.memory = load_memory(task_id)          # é•¿æœŸè®°å¿†
        self.trace = load_trace(task_id)            # è¡Œä¸ºè¿½è¸ª
        self.feedback = load_feedback(task_id)      # åé¦ˆè®°å½•
    
    def get_current_context(self) -> Dict:
        """è·å–å½“å‰å®Œæ•´ä¸Šä¸‹æ–‡"""
        return {
            "short_term": self.context.get_recent_messages(),
            "session_history": self.session.get_history(),
            "relevant_memories": self.memory.get_relevant(),
            "recent_actions": self.trace.get_recent_steps()
        }
```

## å››å¤§æ”¹è¿›æ–¹å‘

### 1. å‘é‡åŒ–æ£€ç´¢ï¼šå¤§è§„æ¨¡ä¸Šä¸‹æ–‡è¯­ä¹‰æ£€ç´¢

#### 1.1 ç°çŠ¶åˆ†æä¸æ”¹è¿›ç›®æ ‡

**ç°æœ‰æ£€ç´¢èƒ½åŠ›**ï¼š
- âœ… **ç²¾ç¡®æ£€ç´¢**ï¼šåŸºäºcontext_idã€trace_idçš„O(1)æ£€ç´¢
- âœ… **åˆ†åŒºæ£€ç´¢**ï¼šcontext_partitionsã€trace_partitionsé«˜æ•ˆç´¢å¼•
- âœ… **ç±»å‹æ£€ç´¢**ï¼šåŸºäºMemoryTypeçš„åˆ†ç±»æ£€ç´¢
- âœ… **æ—¶é—´æ£€ç´¢**ï¼šåŸºäºæ—¶é—´æˆ³çš„å†å²æ£€ç´¢

**æ£€ç´¢å±€é™æ€§**ï¼š
- âŒ **è¯­ä¹‰æ£€ç´¢**ï¼šæ— æ³•å¤„ç†è¯­ä¹‰ç›¸ä¼¼æ€§æŸ¥è¯¢
- âŒ **æ¨¡ç³ŠåŒ¹é…**ï¼šä¸æ”¯æŒå…³é”®è¯æ¨¡ç³ŠåŒ¹é…
- âŒ **æ™ºèƒ½æ’åº**ï¼šç¼ºä¹ç›¸å…³æ€§è¯„åˆ†æœºåˆ¶
- âŒ **è·¨ä¸Šä¸‹æ–‡**ï¼šæ— æ³•è·¨context_idè¯­ä¹‰å…³è”

**æ”¹è¿›ç›®æ ‡**ï¼š
- ğŸ¯ **è¯­ä¹‰æ£€ç´¢**ï¼šåŸºäºå‘é‡ç›¸ä¼¼åº¦çš„è¯­ä¹‰æŸ¥è¯¢
- ğŸ¯ **æ··åˆæ£€ç´¢**ï¼šç»“åˆç²¾ç¡®åŒ¹é…å’Œè¯­ä¹‰åŒ¹é…
- ğŸ¯ **æ™ºèƒ½æ’åº**ï¼šå¤šå› å­ç›¸å…³æ€§è¯„åˆ†
- ğŸ¯ **å¢é‡ç´¢å¼•**ï¼šå®æ—¶å‘é‡åŒ–å’Œç´¢å¼•æ›´æ–°

#### 1.2 åŸºäºç°æœ‰æ¶æ„çš„å¢å¼ºè®¾è®¡

**é›†æˆæ¶æ„è®¾è®¡**ï¼š
```
ç°æœ‰çŠ¶æ€åŸŸå¢å¼º
â”œâ”€â”€ Contextå±‚ (ç°æœ‰context.py)
â”‚   â”œâ”€â”€ æ¶ˆæ¯ç®¡ç† (å·²å®ç°)
â”‚   â”œâ”€â”€ ä¸Šä¸‹æ–‡ç±»å‹ (å·²å®ç°)
â”‚   â””â”€â”€ å‘é‡åŒ–å¢å¼º (æ–°å¢)
â”‚       â”œâ”€â”€ VectorizedContext (æ‰©å±•Contextç±»)
â”‚       â””â”€â”€ SemanticSearch (è¯­ä¹‰æ£€ç´¢æ¥å£)
â”œâ”€â”€ Memoryå±‚ (ç°æœ‰memory_enhanced.py)
â”‚   â”œâ”€â”€ MemoryEntry (å·²å®ç°)
â”‚   â”œâ”€â”€ åˆ†åŒºç´¢å¼• (å·²å®ç°)
â”‚   â””â”€â”€ å‘é‡åŒ–å¢å¼º (æ–°å¢)
â”‚       â”œâ”€â”€ VectorizedMemory (æ‰©å±•EnhancedMemoryç±»)
â”‚       â”œâ”€â”€ EmbeddingManager (å‘é‡ç®¡ç†)
â”‚       â””â”€â”€ SemanticRetrieval (è¯­ä¹‰æ£€ç´¢)
â””â”€â”€ å‘é‡åŒ–æœåŠ¡å±‚ (æ–°å¢æ¨¡å—)
    â”œâ”€â”€ VectorEncoder (å‘é‡ç¼–ç )
    â”œâ”€â”€ VectorStore (å‘é‡å­˜å‚¨)
    â”œâ”€â”€ SemanticMatcher (è¯­ä¹‰åŒ¹é…)
    â””â”€â”€ HybridRetriever (æ··åˆæ£€ç´¢)
```

**ä¸ç°æœ‰ç³»ç»Ÿé›†æˆæ–¹å¼**ï¼š
1. **æ‰©å±•ç°æœ‰ç±»**ï¼šåœ¨Contextå’ŒEnhancedMemoryåŸºç¡€ä¸Šæ·»åŠ å‘é‡åŒ–èƒ½åŠ›
2. **ä¿æŒæ¥å£å…¼å®¹**ï¼šç°æœ‰APIä¸å˜ï¼Œæ–°å¢è¯­ä¹‰æ£€ç´¢API
3. **åˆ©ç”¨ç°æœ‰ç´¢å¼•**ï¼šå¤ç”¨context_partitionså’Œtrace_partitions
4. **é›†æˆç›‘æ§ä½“ç³»**ï¼šçº³å…¥ç°æœ‰å¥åº·æ£€æŸ¥å’Œç»Ÿè®¡æœºåˆ¶

**æ ¸å¿ƒç»„ä»¶å®ç°**ï¼š

```python
class VectorizedContext(Context):
    """å‘é‡åŒ–å¢å¼ºçš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    
    def __init__(self, agent_id: str, max_history: int = 100):
        super().__init__(agent_id, max_history)
        self.vector_encoder = VectorEncoder()
        self.semantic_search = SemanticSearch()
        self.message_embeddings = {}  # message_id -> embedding
        
    async def add_message(self, role: str, content: str, metadata: Dict[str, Any] = None):
        """æ·»åŠ æ¶ˆæ¯å¹¶ç”Ÿæˆå‘é‡"""
        # è°ƒç”¨çˆ¶ç±»æ–¹æ³•
        await super().add_message(role, content, metadata)
        
        # ç”Ÿæˆæœ€æ–°æ¶ˆæ¯çš„å‘é‡
        latest_message = self.messages[-1]
        embedding = await self.vector_encoder.encode_message(latest_message)
        self.message_embeddings[latest_message["message_id"]] = embedding
        
    async def semantic_search_messages(self, query: str, top_k: int = 5) -> List[Dict]:
        """è¯­ä¹‰æ£€ç´¢æ¶ˆæ¯"""
        return await self.semantic_search.search(
            query=query,
            messages=self.messages,
            embeddings=self.message_embeddings,
            top_k=top_k
        )

class VectorizedMemory(EnhancedMemory):
    """å‘é‡åŒ–å¢å¼ºçš„å†…å­˜ç®¡ç†å™¨"""
    
    def __init__(self, agent_id: str, ttl: int = 3600):
        super().__init__(agent_id, ttl)
        self.embedding_manager = EmbeddingManager()
        self.semantic_retrieval = SemanticRetrieval()
        self.entry_embeddings = {}  # entry_id -> embedding
        
    async def add_memory(self, content: str, memory_type: MemoryType, 
                        context_id: str, trace_id: Optional[str] = None,
                        metadata: Dict[str, Any] = None, ttl: Optional[int] = None) -> str:
        """æ·»åŠ å†…å­˜å¹¶ç”Ÿæˆå‘é‡"""
        # è°ƒç”¨çˆ¶ç±»æ–¹æ³•
        entry_id = await super().add_memory(content, memory_type, context_id, trace_id, metadata, ttl)
        
        # ç”Ÿæˆå‘é‡å¹¶å­˜å‚¨
        entry = await self.get_memory(entry_id)
        embedding = await self.embedding_manager.encode_memory_entry(entry)
        self.entry_embeddings[entry_id] = embedding
        
        return entry_id
    
    async def semantic_search_memories(self, query: str, context_id: Optional[str] = None, 
                                     limit: int = 5) -> List[MemoryEntry]:
        """è¯­ä¹‰æ£€ç´¢å†…å­˜"""
        return await self.semantic_retrieval.search(
            query=query,
            entries=self.memory_entries,
            embeddings=self.entry_embeddings,
            context_filter=context_id,
            limit=limit
        )

class HybridRetriever:
    """æ··åˆæ£€ç´¢å™¨ - ç»“åˆç²¾ç¡®åŒ¹é…å’Œè¯­ä¹‰åŒ¹é…"""
    
    def __init__(self, vectorized_memory: VectorizedMemory):
        self.vectorized_memory = vectorized_memory
        
    async def hybrid_search(self, query: str, context_id: Optional[str] = None,
                          semantic_weight: float = 0.7, exact_weight: float = 0.3,
                          limit: int = 10) -> List[MemoryEntry]:
        """æ··åˆæ£€ç´¢ - ç»“åˆè¯­ä¹‰å’Œç²¾ç¡®åŒ¹é…"""
        # 1. è¯­ä¹‰æ£€ç´¢
        semantic_results = await self.vectorized_memory.semantic_search_memories(
            query, context_id, limit * 2
        )
        
        # 2. ç²¾ç¡®åŒ¹é…ï¼ˆåŸºäºç°æœ‰get_relevant_memoriesï¼‰
        exact_results = await self.vectorized_memory.get_relevant_memories(
            query, context_id, limit * 2
        )
        
        # 3. ç»“æœèåˆå’Œé‡æ’åº
        combined_results = self._merge_and_rank_results(
            semantic_results, exact_results, semantic_weight, exact_weight
        )
        
        return combined_results[:limit]
    
    def _merge_and_rank_results(self, semantic_results: List[MemoryEntry], 
                               exact_results: List[MemoryEntry],
                               semantic_weight: float, exact_weight: float) -> List[MemoryEntry]:
        """åˆå¹¶å’Œé‡æ’åºç»“æœ"""
        # å®ç°ç»“æœåˆå¹¶é€»è¾‘
        # åŸºäºç°æœ‰çš„access_countã€created_atç­‰å­—æ®µè¿›è¡Œç»¼åˆè¯„åˆ†
        pass
```

**æ€§èƒ½ä¼˜åŒ–ç­–ç•¥**ï¼š
- **åˆ†å±‚ç´¢å¼•**ï¼šæŒ‰ä¸Šä¸‹æ–‡ç±»å‹å’Œæ—¶é—´åˆ†å±‚å»ºç«‹ç´¢å¼•
- **å¢é‡æ›´æ–°**ï¼šæ”¯æŒä¸Šä¸‹æ–‡å˜åŒ–çš„å¢é‡å‘é‡æ›´æ–°
- **ç¼“å­˜æœºåˆ¶**ï¼šçƒ­ç‚¹ä¸Šä¸‹æ–‡å‘é‡ç¼“å­˜
- **å¹¶è¡Œè®¡ç®—**ï¼šæ‰¹é‡å‘é‡åŒ–å’Œå¹¶è¡Œæ£€ç´¢

#### 1.3 åŸºäºç°æœ‰æ¶æ„çš„å®æ–½è®¡åˆ’

**ç¬¬ä¸€é˜¶æ®µï¼ˆ2-3å‘¨ï¼‰ï¼šæ‰©å±•ç°æœ‰ç±»**
- [ ] åˆ›å»ºVectorizedContextæ‰©å±•ç°æœ‰Contextç±»
- [ ] åˆ›å»ºVectorizedMemoryæ‰©å±•ç°æœ‰EnhancedMemoryç±»  
- [ ] å®ç°VectorEncoderå’ŒEmbeddingManager
- [ ] é›†æˆå‘é‡æ•°æ®åº“ï¼ˆæ¨èChromaï¼Œè½»é‡çº§ï¼‰
- [ ] ä¿æŒç°æœ‰APIå…¼å®¹æ€§ï¼Œæ–°å¢è¯­ä¹‰æ£€ç´¢æ¥å£

**ç¬¬äºŒé˜¶æ®µï¼ˆ2-3å‘¨ï¼‰ï¼šè¯­ä¹‰æ£€ç´¢å®ç°**
- [ ] å®ç°SemanticSearchå’ŒSemanticRetrieval
- [ ] å¼€å‘HybridRetrieveræ··åˆæ£€ç´¢å™¨
- [ ] åˆ©ç”¨ç°æœ‰åˆ†åŒºç´¢å¼•ä¼˜åŒ–æ£€ç´¢æ€§èƒ½
- [ ] é›†æˆç°æœ‰å¥åº·æ£€æŸ¥å’Œç»Ÿè®¡æœºåˆ¶

**ç¬¬ä¸‰é˜¶æ®µï¼ˆ1-2å‘¨ï¼‰ï¼šç³»ç»Ÿé›†æˆä¼˜åŒ–**
- [ ] ä¸ç°æœ‰ç›‘æ§ä½“ç³»é›†æˆï¼ˆåˆ©ç”¨ç°æœ‰statsæœºåˆ¶ï¼‰
- [ ] æ€§èƒ½è°ƒä¼˜ï¼ˆå¤ç”¨ç°æœ‰ç¼“å­˜å’Œåˆ†åŒºæœºåˆ¶ï¼‰
- [ ] å‘åå…¼å®¹æ€§æµ‹è¯•
- [ ] æ–‡æ¡£æ›´æ–°å’Œä½¿ç”¨ç¤ºä¾‹

### 2. å‹ç¼©ç®—æ³•ï¼šé•¿ä¸Šä¸‹æ–‡æ™ºèƒ½å‹ç¼©

#### 2.1 ç°çŠ¶åˆ†æä¸æ”¹è¿›ç›®æ ‡

**ç°æœ‰å‹ç¼©èƒ½åŠ›**ï¼š
- âœ… **å†å²é™åˆ¶**ï¼šmax_historyå‚æ•°æ§åˆ¶æ¶ˆæ¯æ•°é‡
- âœ… **è‡ªåŠ¨æ¸…ç†**ï¼šåŸºäºTTLçš„è¿‡æœŸæ•°æ®æ¸…ç†
- âœ… **åˆ†åŒºç®¡ç†**ï¼šcontext_partitionså’Œtrace_partitionsåˆ†ç¦»å­˜å‚¨
- âœ… **å¿«ç…§æœºåˆ¶**ï¼šSessionSnapshotæ”¯æŒçŠ¶æ€å‹ç¼©

**é•¿ä¸Šä¸‹æ–‡æŒ‘æˆ˜**ï¼š
- âŒ **æ™ºèƒ½å‹ç¼©**ï¼šä»…æ”¯æŒç®€å•çš„æ•°é‡æˆªæ–­ï¼Œæ— æ™ºèƒ½å‹ç¼©
- âŒ **é‡è¦æ€§è¯„ä¼°**ï¼šæ— æ³•è¯†åˆ«é‡è¦æ¶ˆæ¯å’Œå†—ä½™ä¿¡æ¯
- âŒ **è¯­ä¹‰å‹ç¼©**ï¼šç¼ºä¹åŸºäºè¯­ä¹‰çš„å‹ç¼©ç­–ç•¥
- âŒ **åŠ¨æ€ç­–ç•¥**ï¼šå‹ç¼©ç­–ç•¥å›ºå®šï¼Œæ— æ³•æ ¹æ®å†…å®¹è°ƒæ•´

**æ”¹è¿›ç›®æ ‡**ï¼š
- ğŸ¯ **æ™ºèƒ½è¯„ä¼°**ï¼šåŸºäºé‡è¦æ€§è¯„åˆ†çš„æ™ºèƒ½å‹ç¼©
- ğŸ¯ **å¤šå±‚å‹ç¼©**ï¼šç»“åˆç°æœ‰max_historyçš„å¤šå±‚å‹ç¼©ç­–ç•¥
- ğŸ¯ **è¯­ä¹‰ä¿ç•™**ï¼šä¿æŒå…³é”®ä¿¡æ¯çš„è¯­ä¹‰å®Œæ•´æ€§
- ğŸ¯ **æ¸è¿›å‹ç¼©**ï¼šä¸ç°æœ‰TTLæœºåˆ¶ç»“åˆçš„æ¸è¿›å¼å‹ç¼©

#### 2.2 åŸºäºç°æœ‰æ¶æ„çš„å‹ç¼©å¢å¼ºè®¾è®¡

**é›†æˆå‹ç¼©æ¶æ„**ï¼š
```
ç°æœ‰çŠ¶æ€åŸŸå‹ç¼©å¢å¼º
â”œâ”€â”€ Contextå±‚ (ç°æœ‰context.py)
â”‚   â”œâ”€â”€ æ¶ˆæ¯ç®¡ç† (å·²å®ç°)
â”‚   â”œâ”€â”€ max_historyé™åˆ¶ (å·²å®ç°)
â”‚   â””â”€â”€ æ™ºèƒ½å‹ç¼©å¢å¼º (æ–°å¢)
â”‚       â”œâ”€â”€ IntelligentContext (æ‰©å±•Contextç±»)
â”‚       â””â”€â”€ MessageCompressor (æ¶ˆæ¯å‹ç¼©å™¨)
â”œâ”€â”€ Memoryå±‚ (ç°æœ‰memory_enhanced.py)  
â”‚   â”œâ”€â”€ TTLæ¸…ç†æœºåˆ¶ (å·²å®ç°)
â”‚   â”œâ”€â”€ åˆ†åŒºç®¡ç† (å·²å®ç°)
â”‚   â””â”€â”€ å‹ç¼©å¢å¼º (æ–°å¢)
â”‚       â”œâ”€â”€ CompressedMemory (æ‰©å±•EnhancedMemoryç±»)
â”‚       â””â”€â”€ MemoryCompressor (å†…å­˜å‹ç¼©å™¨)
â”œâ”€â”€ Sessionå±‚ (ç°æœ‰session.py)
â”‚   â”œâ”€â”€ å¿«ç…§æœºåˆ¶ (å·²å®ç°)
â”‚   â”œâ”€â”€ çŠ¶æ€ç®¡ç† (å·²å®ç°)
â”‚   â””â”€â”€ å‹ç¼©å¢å¼º (æ–°å¢)
â”‚       â”œâ”€â”€ CompressedSession (æ‰©å±•Sessionç±»)
â”‚       â””â”€â”€ SessionCompressor (ä¼šè¯å‹ç¼©å™¨)
â””â”€â”€ å‹ç¼©æœåŠ¡å±‚ (æ–°å¢æ¨¡å—)
    â”œâ”€â”€ ImportanceEvaluator (é‡è¦æ€§è¯„ä¼°)
    â”œâ”€â”€ SummaryGenerator (æ‘˜è¦ç”Ÿæˆ)
    â”œâ”€â”€ CompressionStrategy (å‹ç¼©ç­–ç•¥)
    â””â”€â”€ CompressionManager (å‹ç¼©ç®¡ç†)
```

**ä¸ç°æœ‰æœºåˆ¶é›†æˆ**ï¼š
1. **æ‰©å±•max_historyæœºåˆ¶**ï¼šåœ¨è¾¾åˆ°é™åˆ¶å‰è¿›è¡Œæ™ºèƒ½å‹ç¼©
2. **å¢å¼ºTTLæ¸…ç†**ï¼šç»“åˆé‡è¦æ€§è¯„ä¼°çš„æ™ºèƒ½æ¸…ç†
3. **ä¼˜åŒ–å¿«ç…§æœºåˆ¶**ï¼šå‹ç¼©å¿«ç…§æ•°æ®å‡å°‘å­˜å‚¨å¼€é”€
4. **ä¿æŒåˆ†åŒºä¼˜åŠ¿**ï¼šåˆ©ç”¨ç°æœ‰åˆ†åŒºç´¢å¼•ä¼˜åŒ–å‹ç¼©æ€§èƒ½

**æ ¸å¿ƒå®ç°**ï¼š

```python
class IntelligentContext(Context):
    """æ™ºèƒ½å‹ç¼©å¢å¼ºçš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    
    def __init__(self, agent_id: str, max_history: int = 100, compression_ratio: float = 0.7):
        super().__init__(agent_id, max_history)
        self.compression_ratio = compression_ratio
        self.message_compressor = MessageCompressor()
        self.importance_evaluator = ImportanceEvaluator()
        self.compressed_messages = []  # å‹ç¼©åçš„æ¶ˆæ¯å­˜å‚¨
        
    async def add_message(self, role: str, content: str, metadata: Dict[str, Any] = None):
        """æ·»åŠ æ¶ˆæ¯å¹¶è§¦å‘æ™ºèƒ½å‹ç¼©"""
        # è°ƒç”¨çˆ¶ç±»æ–¹æ³•
        await super().add_message(role, content, metadata)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å‹ç¼©
        if len(self.messages) >= self.max_history * 0.9:  # è¾¾åˆ°90%æ—¶å¼€å§‹å‹ç¼©
            await self._intelligent_compress()
    
    async def _intelligent_compress(self):
        """æ™ºèƒ½å‹ç¼©æ¶ˆæ¯å†å²"""
        # 1. è¯„ä¼°æ¶ˆæ¯é‡è¦æ€§
        importance_scores = await self.importance_evaluator.evaluate_messages(self.messages)
        
        # 2. é€‰æ‹©è¦å‹ç¼©çš„æ¶ˆæ¯
        target_count = int(len(self.messages) * self.compression_ratio)
        messages_to_compress = self._select_messages_for_compression(
            self.messages, importance_scores, target_count
        )
        
        # 3. æ‰§è¡Œå‹ç¼©
        compressed = await self.message_compressor.compress_messages(messages_to_compress)
        
        # 4. æ›´æ–°æ¶ˆæ¯åˆ—è¡¨
        self._update_message_list(compressed)
        
        logger.info("Context compressed", 
                   original_count=len(self.messages), 
                   compressed_count=len(compressed))

class CompressedMemory(EnhancedMemory):
    """å‹ç¼©å¢å¼ºçš„å†…å­˜ç®¡ç†å™¨"""
    
    def __init__(self, agent_id: str, ttl: int = 3600, compression_threshold: int = 1000):
        super().__init__(agent_id, ttl)
        self.compression_threshold = compression_threshold
        self.memory_compressor = MemoryCompressor()
        self.compressed_entries = {}  # entry_id -> compressed_data
        
    async def add_memory(self, content: str, memory_type: MemoryType, 
                        context_id: str, trace_id: Optional[str] = None,
                        metadata: Dict[str, Any] = None, ttl: Optional[int] = None) -> str:
        """æ·»åŠ å†…å­˜å¹¶æ£€æŸ¥å‹ç¼©éœ€æ±‚"""
        # è°ƒç”¨çˆ¶ç±»æ–¹æ³•
        entry_id = await super().add_memory(content, memory_type, context_id, trace_id, metadata, ttl)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å‹ç¼©
        if len(self.memory_entries) >= self.compression_threshold:
            await self._compress_old_memories()
        
        return entry_id
    
    async def _compress_old_memories(self):
        """å‹ç¼©æ—§çš„å†…å­˜æ¡ç›®"""
        # 1. æŒ‰æ—¶é—´æ’åºï¼Œè·å–æœ€æ—§çš„æ¡ç›®
        sorted_entries = sorted(self.memory_entries, key=lambda x: x.created_at)
        old_entries = sorted_entries[:len(sorted_entries) // 2]  # å‹ç¼©ä¸€åŠ
        
        # 2. è¯„ä¼°é‡è¦æ€§
        importance_scores = await self.importance_evaluator.evaluate_memories(old_entries)
        
        # 3. å‹ç¼©ä½é‡è¦æ€§æ¡ç›®
        for entry in old_entries:
            if importance_scores.get(entry.entry_id, 0) < 0.5:  # ä½é‡è¦æ€§é˜ˆå€¼
                compressed_data = await self.memory_compressor.compress_entry(entry)
                self.compressed_entries[entry.entry_id] = compressed_data
                self.memory_entries.remove(entry)
        
        # 4. æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self._update_stats()

class CompressedSession(Session):
    """å‹ç¼©å¢å¼ºçš„ä¼šè¯ç®¡ç†å™¨"""
    
    def __init__(self, context_id: str, agent_id: str, tenant_id: str = "default", 
                 timeout: Optional[int] = None, max_snapshots: int = 10):
        super().__init__(context_id, agent_id, tenant_id, timeout)
        self.max_snapshots = max_snapshots
        self.session_compressor = SessionCompressor()
        self.compressed_snapshots = {}  # snapshot_id -> compressed_data
    
    def create_snapshot(self) -> SessionSnapshot:
        """åˆ›å»ºå‹ç¼©å¿«ç…§"""
        # è°ƒç”¨çˆ¶ç±»æ–¹æ³•åˆ›å»ºå¿«ç…§
        snapshot = super().create_snapshot()
        
        # å¦‚æœå¿«ç…§è¿‡å¤šï¼Œå‹ç¼©æ—§å¿«ç…§
        if len(self.snapshots) >= self.max_snapshots:
            self._compress_old_snapshots()
        
        return snapshot
    
    def _compress_old_snapshots(self):
        """å‹ç¼©æ—§å¿«ç…§"""
        # ä¿ç•™æœ€æ–°çš„å‡ ä¸ªå¿«ç…§ï¼Œå‹ç¼©å…¶ä½™çš„
        snapshots_to_compress = self.snapshots[:-self.max_snapshots//2]
        
        for snapshot in snapshots_to_compress:
            compressed_data = self.session_compressor.compress_snapshot(snapshot)
            self.compressed_snapshots[snapshot.snapshot_id] = compressed_data
            self.snapshots.remove(snapshot)
```

**å‹ç¼©ç­–ç•¥å®ç°**ï¼š
```python
class ImportanceEvaluator:
    """åŸºäºç°æœ‰æ•°æ®ç»“æ„çš„é‡è¦æ€§è¯„ä¼°å™¨"""
    
    def __init__(self):
        self.temporal_weight = 0.3
        self.semantic_weight = 0.4
        self.interaction_weight = 0.3
        
    async def evaluate_messages(self, messages: List[Dict]) -> Dict[str, float]:
        """è¯„ä¼°æ¶ˆæ¯é‡è¦æ€§ - åˆ©ç”¨ç°æœ‰messageç»“æ„"""
        importance_scores = {}
        
        for msg in messages:
            # 1. æ—¶é—´è¡°å‡è¯„åˆ† - åˆ©ç”¨ç°æœ‰timestampå­—æ®µ
            temporal_score = self._calculate_temporal_score(msg)
            
            # 2. å†…å®¹é‡è¦æ€§è¯„åˆ† - åŸºäºç°æœ‰contentå’Œmetadata
            content_score = self._calculate_content_score(msg)
            
            # 3. äº¤äº’é‡è¦æ€§è¯„åˆ† - åŸºäºroleå’Œtrace_id
            interaction_score = self._calculate_interaction_score(msg)
            
            # 4. ç»¼åˆè¯„åˆ†
            final_score = (
                temporal_score * self.temporal_weight +
                content_score * self.semantic_weight +
                interaction_score * self.interaction_weight
            )
            
            importance_scores[msg["message_id"]] = final_score
        
        return importance_scores
    
    async def evaluate_memories(self, entries: List[MemoryEntry]) -> Dict[str, float]:
        """è¯„ä¼°å†…å­˜æ¡ç›®é‡è¦æ€§ - åˆ©ç”¨ç°æœ‰MemoryEntryç»“æ„"""
        importance_scores = {}
        
        for entry in entries:
            # 1. è®¿é—®é¢‘ç‡è¯„åˆ† - åˆ©ç”¨ç°æœ‰access_count
            access_score = min(entry.access_count / 10.0, 1.0)  # æ ‡å‡†åŒ–åˆ°0-1
            
            # 2. æ—¶é—´è¡°å‡è¯„åˆ† - åˆ©ç”¨ç°æœ‰created_atå’Œlast_accessed
            temporal_score = self._calculate_memory_temporal_score(entry)
            
            # 3. ç±»å‹é‡è¦æ€§è¯„åˆ† - åŸºäºç°æœ‰memory_type
            type_score = self._calculate_type_score(entry.memory_type)
            
            # 4. ç»¼åˆè¯„åˆ†
            final_score = (access_score + temporal_score + type_score) / 3
            importance_scores[entry.entry_id] = final_score
        
        return importance_scores
```

**å‹ç¼©ç­–ç•¥**ï¼š
1. **å±‚æ¬¡åŒ–å‹ç¼©**ï¼šæŒ‰é‡è¦æ€§åˆ†å±‚ï¼Œä¸åŒå±‚çº§é‡‡ç”¨ä¸åŒå‹ç¼©ç‡
2. **è¯­ä¹‰å‹ç¼©**ï¼šä¿ç•™è¯­ä¹‰æ ¸å¿ƒï¼Œå‹ç¼©å†—ä½™è¡¨è¾¾
3. **ç»“æ„åŒ–å‹ç¼©**ï¼šä¿æŒå¯¹è¯ç»“æ„ï¼Œå‹ç¼©å…·ä½“å†…å®¹
4. **è‡ªé€‚åº”å‹ç¼©**ï¼šæ ¹æ®ä½¿ç”¨æ¨¡å¼åŠ¨æ€è°ƒæ•´å‹ç¼©ç­–ç•¥

#### 2.3 åŸºäºç°æœ‰æ¶æ„çš„å®æ–½è®¡åˆ’

**ç¬¬ä¸€é˜¶æ®µï¼ˆ2-3å‘¨ï¼‰ï¼šæ‰©å±•ç°æœ‰å‹ç¼©æœºåˆ¶**
- [ ] åˆ›å»ºIntelligentContextæ‰©å±•ç°æœ‰Contextç±»çš„max_historyæœºåˆ¶
- [ ] åˆ›å»ºCompressedMemoryæ‰©å±•ç°æœ‰EnhancedMemoryç±»çš„TTLæœºåˆ¶
- [ ] åˆ›å»ºCompressedSessionæ‰©å±•ç°æœ‰Sessionç±»çš„å¿«ç…§æœºåˆ¶
- [ ] å®ç°ImportanceEvaluatoråŸºäºç°æœ‰æ•°æ®ç»“æ„è¯„ä¼°é‡è¦æ€§

**ç¬¬äºŒé˜¶æ®µï¼ˆ2-3å‘¨ï¼‰ï¼šå‹ç¼©ç®—æ³•é›†æˆ**
- [ ] å®ç°MessageCompressorã€MemoryCompressorã€SessionCompressor
- [ ] é›†æˆåˆ°ç°æœ‰çš„å¥åº·æ£€æŸ¥å’Œç»Ÿè®¡æœºåˆ¶
- [ ] åˆ©ç”¨ç°æœ‰åˆ†åŒºç´¢å¼•ä¼˜åŒ–å‹ç¼©æ€§èƒ½
- [ ] ä¸ç°æœ‰ç›‘æ§ä½“ç³»æ— ç¼é›†æˆ

**ç¬¬ä¸‰é˜¶æ®µï¼ˆ1-2å‘¨ï¼‰ï¼šä¼˜åŒ–å’Œæµ‹è¯•**
- [ ] å‹ç¼©æ•ˆæœè¯„ä¼°å’Œå‚æ•°è°ƒä¼˜
- [ ] ä¸ç°æœ‰APIçš„å…¼å®¹æ€§æµ‹è¯•
- [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼ˆåŸºäºç°æœ‰statsæœºåˆ¶ï¼‰
- [ ] æ–‡æ¡£æ›´æ–°å’Œä½¿ç”¨ç¤ºä¾‹

### 3. åˆ†å¸ƒå¼å­˜å‚¨ï¼šè·¨èŠ‚ç‚¹ä¸Šä¸‹æ–‡åŒæ­¥

#### 3.1 ç°çŠ¶åˆ†æä¸æ”¹è¿›ç›®æ ‡

**ç°æœ‰å­˜å‚¨èƒ½åŠ›**ï¼š
- âœ… **æœ¬åœ°å­˜å‚¨**ï¼šåŸºäºå†…å­˜çš„é«˜æ•ˆå­˜å‚¨å’Œåˆ†åŒºç´¢å¼•
- âœ… **æŒä¹…åŒ–æ”¯æŒ**ï¼šSessionSnapshotå¿«ç…§æœºåˆ¶
- âœ… **å¤šç§Ÿæˆ·éš”ç¦»**ï¼štenant_idéš”ç¦»æœºåˆ¶
- âœ… **å¥åº·ç›‘æ§**ï¼šå®Œæ•´çš„å¥åº·æ£€æŸ¥å’Œç»Ÿè®¡æœºåˆ¶

**åˆ†å¸ƒå¼æŒ‘æˆ˜**ï¼š
- âŒ **å•ç‚¹æ•…éšœ**ï¼šå½“å‰ä¸ºå•èŠ‚ç‚¹å­˜å‚¨ï¼Œå­˜åœ¨å¯ç”¨æ€§é£é™©
- âŒ **æ°´å¹³æ‰©å±•**ï¼šæ— æ³•è·¨å¤šä¸ªèŠ‚ç‚¹æ‰©å±•å­˜å‚¨å®¹é‡
- âŒ **æ•°æ®åŒæ­¥**ï¼šç¼ºä¹è·¨èŠ‚ç‚¹æ•°æ®åŒæ­¥æœºåˆ¶
- âŒ **è´Ÿè½½åˆ†æ‹…**ï¼šæ— æ³•åˆ†æ•£å­˜å‚¨å’Œè®¡ç®—è´Ÿè½½

**æ”¹è¿›ç›®æ ‡**ï¼š
- ğŸ¯ **é«˜å¯ç”¨æ¶æ„**ï¼šåŸºäºç°æœ‰æ¶æ„å®ç°å¤šèŠ‚ç‚¹éƒ¨ç½²
- ğŸ¯ **æ•°æ®å¤åˆ¶**ï¼šåˆ©ç”¨ç°æœ‰å¿«ç…§æœºåˆ¶å®ç°æ•°æ®å¤åˆ¶
- ğŸ¯ **ä¸€è‡´æ€§ä¿è¯**ï¼šåŸºäºç°æœ‰trace_idå®ç°æ•°æ®ä¸€è‡´æ€§
- ğŸ¯ **æ¸è¿›æ‰©å±•**ï¼šä¿æŒç°æœ‰APIå…¼å®¹çš„æ¸è¿›å¼åˆ†å¸ƒå¼æ”¹é€ 

#### 3.2 åŸºäºç°æœ‰æ¶æ„çš„åˆ†å¸ƒå¼å¢å¼ºè®¾è®¡

**é›†æˆåˆ†å¸ƒå¼æ¶æ„**ï¼š
```
ç°æœ‰çŠ¶æ€åŸŸåˆ†å¸ƒå¼å¢å¼º
â”œâ”€â”€ Contextå±‚ (ç°æœ‰context.py)
â”‚   â”œâ”€â”€ åˆ†åŒºç´¢å¼• (å·²å®ç°)
â”‚   â”œâ”€â”€ trace_idç®¡ç† (å·²å®ç°)
â”‚   â””â”€â”€ åˆ†å¸ƒå¼å¢å¼º (æ–°å¢)
â”‚       â”œâ”€â”€ DistributedContext (æ‰©å±•Contextç±»)
â”‚       â””â”€â”€ ContextReplicator (ä¸Šä¸‹æ–‡å¤åˆ¶å™¨)
â”œâ”€â”€ Memoryå±‚ (ç°æœ‰memory_enhanced.py)
â”‚   â”œâ”€â”€ åˆ†åŒºç®¡ç† (å·²å®ç°)
â”‚   â”œâ”€â”€ TTLæœºåˆ¶ (å·²å®ç°)
â”‚   â””â”€â”€ åˆ†å¸ƒå¼å¢å¼º (æ–°å¢)
â”‚       â”œâ”€â”€ DistributedMemory (æ‰©å±•EnhancedMemoryç±»)
â”‚       â””â”€â”€ MemoryReplicator (å†…å­˜å¤åˆ¶å™¨)
â”œâ”€â”€ Sessionå±‚ (ç°æœ‰session.py)
â”‚   â”œâ”€â”€ å¿«ç…§æœºåˆ¶ (å·²å®ç°)
â”‚   â”œâ”€â”€ çŠ¶æ€ç®¡ç† (å·²å®ç°)
â”‚   â””â”€â”€ åˆ†å¸ƒå¼å¢å¼º (æ–°å¢)
â”‚       â”œâ”€â”€ DistributedSession (æ‰©å±•Sessionç±»)
â”‚       â””â”€â”€ SessionReplicator (ä¼šè¯å¤åˆ¶å™¨)
â””â”€â”€ åˆ†å¸ƒå¼åè°ƒå±‚ (æ–°å¢æ¨¡å—)
    â”œâ”€â”€ NodeManager (èŠ‚ç‚¹ç®¡ç†)
    â”œâ”€â”€ ReplicationManager (å¤åˆ¶ç®¡ç†)
    â”œâ”€â”€ ConsistencyManager (ä¸€è‡´æ€§ç®¡ç†)
    â””â”€â”€ LoadBalancer (è´Ÿè½½å‡è¡¡)
```

**ä¸ç°æœ‰æœºåˆ¶é›†æˆ**ï¼š
1. **æ‰©å±•åˆ†åŒºæœºåˆ¶**ï¼šåˆ©ç”¨ç°æœ‰context_partitionså®ç°è·¨èŠ‚ç‚¹åˆ†åŒº
2. **å¢å¼ºå¿«ç…§å¤åˆ¶**ï¼šåŸºäºSessionSnapshotå®ç°è·¨èŠ‚ç‚¹æ•°æ®å¤åˆ¶
3. **ä¿æŒtrace_idä¸€è‡´æ€§**ï¼šåˆ©ç”¨ç°æœ‰trace_idç¡®ä¿æ•°æ®ä¸€è‡´æ€§
4. **å¤ç”¨å¥åº·æ£€æŸ¥**ï¼šæ‰©å±•ç°æœ‰å¥åº·æ£€æŸ¥æœºåˆ¶åˆ°åˆ†å¸ƒå¼ç¯å¢ƒ

**æ ¸å¿ƒå®ç°**ï¼š

```python
class DistributedContextStorage:
    """åˆ†å¸ƒå¼ä¸Šä¸‹æ–‡å­˜å‚¨ç³»ç»Ÿ"""
    
    def __init__(self, cluster_config: Dict[str, Any]):
        self.cluster_config = cluster_config
        self.sharding_manager = ShardingManager(cluster_config)
        self.replication_manager = ReplicationManager(cluster_config)
        self.consistency_manager = ConsistencyManager(cluster_config)
        self.node_manager = NodeManager(cluster_config)
        
    async def initialize_cluster(self) -> None:
        """åˆå§‹åŒ–åˆ†å¸ƒå¼é›†ç¾¤"""
        # 1. åˆå§‹åŒ–èŠ‚ç‚¹
        await self.node_manager.initialize_nodes()
        
        # 2. å»ºç«‹åˆ†ç‰‡ç­–ç•¥
        await self.sharding_manager.setup_sharding()
        
        # 3. é…ç½®å¤åˆ¶ç­–ç•¥
        await self.replication_manager.setup_replication()
        
        # 4. å¯åŠ¨ä¸€è‡´æ€§åè®®
        await self.consistency_manager.start_consensus()
    
    async def store_context(self, context: Context) -> None:
        """åˆ†å¸ƒå¼å­˜å‚¨ä¸Šä¸‹æ–‡"""
        # 1. ç¡®å®šåˆ†ç‰‡ä½ç½®
        shard_key = self._generate_shard_key(context)
        target_nodes = await self.sharding_manager.get_target_nodes(shard_key)
        
        # 2. åºåˆ—åŒ–ä¸Šä¸‹æ–‡æ•°æ®
        serialized_data = await self._serialize_context(context)
        
        # 3. å¹¶è¡Œå†™å…¥å¤šä¸ªèŠ‚ç‚¹
        write_tasks = []
        for node in target_nodes:
            task = self._write_to_node(node, context.context_id, serialized_data)
            write_tasks.append(task)
        
        # 4. ç­‰å¾…å†™å…¥å®Œæˆï¼ˆå¯é…ç½®ä¸€è‡´æ€§çº§åˆ«ï¼‰
        await self._wait_for_writes(write_tasks, consistency_level="quorum")
        
        # 5. æ›´æ–°å…ƒæ•°æ®
        await self._update_metadata(context.context_id, target_nodes)
    
    async def retrieve_context(self, context_id: str) -> Optional[Context]:
        """åˆ†å¸ƒå¼æ£€ç´¢ä¸Šä¸‹æ–‡"""
        # 1. æŸ¥æ‰¾å­˜å‚¨ä½ç½®
        storage_nodes = await self.sharding_manager.locate_context(context_id)
        
        # 2. å¹¶è¡Œè¯»å–
        read_tasks = []
        for node in storage_nodes:
            task = self._read_from_node(node, context_id)
            read_tasks.append(task)
        
        # 3. è·å–è¯»å–ç»“æœ
        results = await asyncio.gather(*read_tasks, return_exceptions=True)
        
        # 4. é€‰æ‹©æœ€æ–°ç‰ˆæœ¬
        latest_context = await self._select_latest_version(results)
        
        return latest_context
    
    async def sync_context_across_nodes(self, context_id: str) -> None:
        """è·¨èŠ‚ç‚¹åŒæ­¥ä¸Šä¸‹æ–‡"""
        # 1. è·å–æ‰€æœ‰å‰¯æœ¬
        all_replicas = await self._get_all_replicas(context_id)
        
        # 2. æ£€æµ‹ç‰ˆæœ¬å†²çª
        conflicts = await self._detect_conflicts(all_replicas)
        
        # 3. è§£å†³å†²çª
        if conflicts:
            resolved_context = await self._resolve_conflicts(conflicts)
            await self._propagate_resolution(context_id, resolved_context)
        
        # 4. ç¡®ä¿ä¸€è‡´æ€§
        await self._ensure_consistency(context_id)

class ShardingManager:
    """åˆ†ç‰‡ç®¡ç†å™¨"""
    
    def __init__(self, cluster_config: Dict[str, Any]):
        self.cluster_config = cluster_config
        self.shard_map = {}
        self.hash_ring = ConsistentHashRing()
        
    async def setup_sharding(self) -> None:
        """è®¾ç½®åˆ†ç‰‡ç­–ç•¥"""
        # 1. åˆå§‹åŒ–ä¸€è‡´æ€§å“ˆå¸Œç¯
        for node in self.cluster_config["nodes"]:
            self.hash_ring.add_node(node["id"], node["weight"])
        
        # 2. åˆ›å»ºåˆ†ç‰‡æ˜ å°„
        await self._create_shard_mapping()
        
        # 3. å¹³è¡¡è´Ÿè½½
        await self._balance_shards()
    
    def _generate_shard_key(self, context: Context) -> str:
        """ç”Ÿæˆåˆ†ç‰‡é”®"""
        # åŸºäºcontext_idã€agent_idã€tenant_idç­‰ç”Ÿæˆåˆ†ç‰‡é”®
        shard_components = [
            context.context_id,
            context.agent_id,
            getattr(context, 'tenant_id', 'default')
        ]
        return hashlib.md5("|".join(shard_components).encode()).hexdigest()
    
    async def get_target_nodes(self, shard_key: str) -> List[str]:
        """è·å–ç›®æ ‡å­˜å‚¨èŠ‚ç‚¹"""
        # 1. ä¸»èŠ‚ç‚¹
        primary_node = self.hash_ring.get_node(shard_key)
        
        # 2. å‰¯æœ¬èŠ‚ç‚¹
        replica_nodes = self.hash_ring.get_replica_nodes(
            shard_key, 
            replica_count=self.cluster_config.get("replica_count", 2)
        )
        
        return [primary_node] + replica_nodes
```

**ä¸€è‡´æ€§ç­–ç•¥**ï¼š
- **å¼ºä¸€è‡´æ€§**ï¼šå…³é”®ä¸Šä¸‹æ–‡ä½¿ç”¨Raft/Paxosåè®®
- **æœ€ç»ˆä¸€è‡´æ€§**ï¼šä¸€èˆ¬ä¸Šä¸‹æ–‡ä½¿ç”¨å¼‚æ­¥å¤åˆ¶
- **ä¼šè¯ä¸€è‡´æ€§**ï¼šåŒä¸€ä¼šè¯å†…ä¿è¯è¯»å†™ä¸€è‡´æ€§
- **å› æœä¸€è‡´æ€§**ï¼šä¿è¯å› æœå…³ç³»çš„æ¶ˆæ¯é¡ºåº

#### 3.3 å®æ–½è®¡åˆ’

**ç¬¬ä¸€é˜¶æ®µï¼ˆ4-5å‘¨ï¼‰ï¼šåŸºç¡€åˆ†å¸ƒå¼æ¶æ„**
- [ ] å®ç°ShardingManagerå’ŒReplicationManager
- [ ] ä¸€è‡´æ€§å“ˆå¸Œå’Œåˆ†ç‰‡ç­–ç•¥
- [ ] åŸºç¡€è¯»å†™æ“ä½œ
- [ ] æ•…éšœæ£€æµ‹å’Œæ¢å¤

**ç¬¬äºŒé˜¶æ®µï¼ˆ3-4å‘¨ï¼‰ï¼šä¸€è‡´æ€§å’ŒåŒæ­¥**
- [ ] å®ç°ä¸€è‡´æ€§åè®®
- [ ] å†²çªæ£€æµ‹å’Œè§£å†³
- [ ] è·¨èŠ‚ç‚¹åŒæ­¥æœºåˆ¶
- [ ] æ€§èƒ½ä¼˜åŒ–

**ç¬¬ä¸‰é˜¶æ®µï¼ˆ2-3å‘¨ï¼‰ï¼šè¿ç»´å’Œç›‘æ§**
- [ ] é›†ç¾¤ç®¡ç†å·¥å…·
- [ ] ç›‘æ§å’Œå‘Šè­¦ç³»ç»Ÿ
- [ ] è‡ªåŠ¨åŒ–éƒ¨ç½²
- [ ] å®¹ç¾æ¢å¤

### 4. TraceWriterå¢å¼ºï¼šä¼ä¸šçº§è¡Œä¸ºè¿½è¸ªä¸è°ƒè¯•ä½“ç³»

#### 4.1 ç°çŠ¶åˆ†æä¸æ”¹è¿›ç›®æ ‡

**ç°æœ‰è¿½è¸ªèƒ½åŠ›**ï¼š
- âœ… **åŸºç¡€è®°å½•**ï¼šTraceWriteræ”¯æŒåŸºæœ¬çš„æ‰§è¡Œè®°å½•
- âœ… **trace_idå…³è”**ï¼šå®Œæ•´çš„trace_id + context_idå…³è”æœºåˆ¶
- âœ… **å¼‚æ­¥å†™å…¥**ï¼šæ”¯æŒå¼‚æ­¥æ—¥å¿—å†™å…¥ï¼Œä¸å½±å“æ‰§è¡Œæ€§èƒ½
- âœ… **ç»“æ„åŒ–å­˜å‚¨**ï¼šåŸºäºå­—å…¸ç»“æ„çš„çµæ´»æ—¥å¿—æ ¼å¼

**è¿½è¸ªå±€é™æ€§**ï¼š
- âŒ **è¡Œä¸ºå›æ”¾**ï¼šç¼ºä¹å®Œæ•´çš„è¡Œä¸ºé“¾å›æ”¾æœºåˆ¶
- âŒ **è°ƒè¯•å·¥å…·**ï¼šæ— å¯è§†åŒ–è°ƒè¯•å’Œåˆ†æå·¥å…·
- âŒ **å¤±è´¥åˆ†æ**ï¼šç¼ºä¹ç³»ç»Ÿæ€§çš„å¤±è´¥åŸå› åˆ†æ
- âŒ **æ€§èƒ½åˆ†æ**ï¼šæ— è¯¦ç»†çš„æ€§èƒ½ç“¶é¢ˆåˆ†æèƒ½åŠ›

**æ”¹è¿›ç›®æ ‡**ï¼š
- ğŸ¯ **å®Œæ•´å›æ”¾**ï¼šæ”¯æŒä»»åŠ¡æ‰§è¡Œé“¾çš„å®Œæ•´é‡ç°
- ğŸ¯ **å¯è§†åŒ–è°ƒè¯•**ï¼šæä¾›ç›´è§‚çš„è°ƒè¯•å’Œåˆ†æç•Œé¢
- ğŸ¯ **æ™ºèƒ½åˆ†æ**ï¼šè‡ªåŠ¨è¯†åˆ«å¤±è´¥åŸå› å’Œæ€§èƒ½ç“¶é¢ˆ
- ğŸ¯ **å®¡è®¡æ”¯æŒ**ï¼šæ»¡è¶³ä¼ä¸šçº§å®¡è®¡å’Œåˆè§„è¦æ±‚

#### 4.2 åŸºäºç°æœ‰TraceWriterçš„å¢å¼ºè®¾è®¡

**é›†æˆè¿½è¸ªæ¶æ„**ï¼š
```
ç°æœ‰TraceWriterå¢å¼º
â”œâ”€â”€ åŸºç¡€è¿½è¸ª (ç°æœ‰trace_writer.py)
â”‚   â”œâ”€â”€ å¼‚æ­¥å†™å…¥ (å·²å®ç°)
â”‚   â”œâ”€â”€ trace_idå…³è” (å·²å®ç°)
â”‚   â””â”€â”€ ç»“æ„åŒ–å­˜å‚¨ (å·²å®ç°)
â”œâ”€â”€ è¿½è¸ªå¢å¼ºå±‚ (æ–°å¢)
â”‚   â”œâ”€â”€ EnhancedTraceWriter (æ‰©å±•ç°æœ‰TraceWriter)
â”‚   â”œâ”€â”€ TraceAnalyzer (è¿½è¸ªåˆ†æå™¨)
â”‚   â”œâ”€â”€ TraceReplayer (å›æ”¾å™¨)
â”‚   â””â”€â”€ TraceVisualizer (å¯è§†åŒ–å·¥å…·)
â””â”€â”€ ä¼ä¸šçº§ç‰¹æ€§ (æ–°å¢)
    â”œâ”€â”€ AuditTraceWriter (å®¡è®¡è¿½è¸ª)
    â”œâ”€â”€ PerformanceAnalyzer (æ€§èƒ½åˆ†æ)
    â”œâ”€â”€ FailureAnalyzer (å¤±è´¥åˆ†æ)
    â””â”€â”€ ComplianceReporter (åˆè§„æŠ¥å‘Š)
```

**æ ¸å¿ƒç»„ä»¶å®ç°**ï¼š

```python
class EnhancedTraceWriter(TraceWriter):
    """å¢å¼ºçš„è¿½è¸ªå†™å…¥å™¨"""
    
    def __init__(self):
        super().__init__()
        self.trace_analyzer = TraceAnalyzer()
        self.performance_metrics = {}
        self.failure_patterns = {}
        
    async def write_step(self, trace_id: str, step_data: Dict[str, Any]):
        """å†™å…¥æ‰§è¡Œæ­¥éª¤å¹¶è¿›è¡Œå®æ—¶åˆ†æ"""
        # è°ƒç”¨çˆ¶ç±»æ–¹æ³•
        await super().write_step(trace_id, step_data)
        
        # å®æ—¶æ€§èƒ½åˆ†æ
        await self._analyze_performance(trace_id, step_data)
        
        # å¤±è´¥æ¨¡å¼æ£€æµ‹
        if not step_data.get("success", True):
            await self._analyze_failure(trace_id, step_data)
    
    async def _analyze_performance(self, trace_id: str, step_data: Dict):
        """å®æ—¶æ€§èƒ½åˆ†æ"""
        duration = step_data.get("duration", 0)
        step_type = step_data.get("step_type", "unknown")
        
        # æ›´æ–°æ€§èƒ½æŒ‡æ ‡
        if trace_id not in self.performance_metrics:
            self.performance_metrics[trace_id] = {}
        
        metrics = self.performance_metrics[trace_id]
        metrics.setdefault(step_type, []).append(duration)
        
        # æ£€æµ‹æ€§èƒ½å¼‚å¸¸
        if duration > self._get_threshold(step_type):
            await self._record_performance_anomaly(trace_id, step_data)

class TraceReplayer:
    """è¿½è¸ªå›æ”¾å™¨ - æ”¯æŒå®Œæ•´ä»»åŠ¡é‡ç°"""
    
    def __init__(self, trace_writer: EnhancedTraceWriter):
        self.trace_writer = trace_writer
        self.replay_mode = "debug"  # debug, audit, analysis
        
    async def replay_trace(self, trace_id: str, start_step: int = 0, 
                          end_step: Optional[int] = None) -> Dict:
        """å›æ”¾æŒ‡å®šè¿½è¸ªé“¾"""
        trace_data = await self.trace_writer.get_trace(trace_id)
        
        if not trace_data:
            raise ValueError(f"Trace {trace_id} not found")
        
        # é€‰æ‹©å›æ”¾èŒƒå›´
        steps = trace_data[start_step:end_step]
        
        replay_result = {
            "trace_id": trace_id,
            "replay_mode": self.replay_mode,
            "steps_replayed": len(steps),
            "replay_log": []
        }
        
        for i, step in enumerate(steps):
            step_result = await self._replay_step(step, i + start_step)
            replay_result["replay_log"].append(step_result)
            
            # è°ƒè¯•æ¨¡å¼ä¸‹å¯ä»¥æš‚åœ
            if self.replay_mode == "debug":
                await self._debug_pause_check(step, step_result)
        
        return replay_result
    
    async def _replay_step(self, step_data: Dict, step_index: int) -> Dict:
        """å›æ”¾å•ä¸ªæ­¥éª¤"""
        return {
            "step_index": step_index,
            "original_step": step_data,
            "replay_timestamp": datetime.utcnow().isoformat(),
            "replay_status": "completed",
            "differences": await self._compare_with_original(step_data)
        }

class TraceAnalyzer:
    """è¿½è¸ªåˆ†æå™¨ - æ™ºèƒ½åˆ†ææ‰§è¡Œæ¨¡å¼"""
    
    def __init__(self):
        self.pattern_detector = PatternDetector()
        self.anomaly_detector = AnomalyDetector()
        
    async def analyze_trace_pattern(self, trace_id: str) -> Dict:
        """åˆ†æè¿½è¸ªæ¨¡å¼"""
        trace_data = await self._get_trace_data(trace_id)
        
        analysis_result = {
            "trace_id": trace_id,
            "execution_pattern": await self._detect_execution_pattern(trace_data),
            "performance_analysis": await self._analyze_performance_pattern(trace_data),
            "failure_analysis": await self._analyze_failure_pattern(trace_data),
            "optimization_suggestions": await self._generate_optimization_suggestions(trace_data)
        }
        
        return analysis_result
    
    async def _detect_execution_pattern(self, trace_data: List[Dict]) -> Dict:
        """æ£€æµ‹æ‰§è¡Œæ¨¡å¼"""
        patterns = {
            "sequential": self._is_sequential_pattern(trace_data),
            "parallel": self._is_parallel_pattern(trace_data),
            "recursive": self._is_recursive_pattern(trace_data),
            "retry": self._is_retry_pattern(trace_data)
        }
        
        return {
            "detected_patterns": [k for k, v in patterns.items() if v],
            "execution_flow": self._analyze_execution_flow(trace_data),
            "decision_points": self._identify_decision_points(trace_data)
        }

class AuditTraceWriter(EnhancedTraceWriter):
    """å®¡è®¡çº§è¿½è¸ªå†™å…¥å™¨"""
    
    def __init__(self):
        super().__init__()
        self.audit_level = "enterprise"
        self.compliance_checker = ComplianceChecker()
        
    async def write_audit_step(self, trace_id: str, step_data: Dict, 
                              audit_metadata: Dict = None):
        """å†™å…¥å®¡è®¡çº§æ­¥éª¤è®°å½•"""
        # å¢å¼ºæ­¥éª¤æ•°æ®
        enhanced_step = {
            **step_data,
            "audit_timestamp": datetime.utcnow().isoformat(),
            "audit_level": self.audit_level,
            "compliance_tags": await self._generate_compliance_tags(step_data),
            "data_sensitivity": await self._classify_data_sensitivity(step_data),
            "audit_metadata": audit_metadata or {}
        }
        
        # åˆè§„æ€§æ£€æŸ¥
        compliance_result = await self.compliance_checker.check(enhanced_step)
        enhanced_step["compliance_status"] = compliance_result
        
        # å†™å…¥å®¡è®¡æ—¥å¿—
        await super().write_step(trace_id, enhanced_step)
        
        # å¦‚æœæœ‰åˆè§„é—®é¢˜ï¼Œè§¦å‘å‘Šè­¦
        if not compliance_result.get("compliant", True):
            await self._trigger_compliance_alert(trace_id, enhanced_step)
```

#### 4.3 ä¼ä¸šçº§è°ƒè¯•ä¸åˆ†æå·¥å…·

**è°ƒè¯•å·¥å…·é›†æˆ**ï¼š
```python
class TraceDebugger:
    """è¿½è¸ªè°ƒè¯•å™¨"""
    
    def __init__(self, trace_replayer: TraceReplayer):
        self.replayer = trace_replayer
        self.breakpoints = {}
        self.watch_variables = {}
        
    async def set_breakpoint(self, trace_id: str, step_index: int, 
                           condition: Optional[str] = None):
        """è®¾ç½®æ–­ç‚¹"""
        self.breakpoints.setdefault(trace_id, []).append({
            "step_index": step_index,
            "condition": condition,
            "created_at": datetime.utcnow().isoformat()
        })
    
    async def debug_trace(self, trace_id: str) -> Dict:
        """è°ƒè¯•æ¨¡å¼æ‰§è¡Œè¿½è¸ª"""
        self.replayer.replay_mode = "debug"
        return await self.replayer.replay_trace(trace_id)
    
    async def analyze_failure_point(self, trace_id: str, 
                                  failure_step: int) -> Dict:
        """åˆ†æå¤±è´¥ç‚¹"""
        # è·å–å¤±è´¥å‰åçš„ä¸Šä¸‹æ–‡
        context_before = await self._get_context_before_failure(
            trace_id, failure_step
        )
        context_after = await self._get_context_after_failure(
            trace_id, failure_step
        )
        
        return {
            "failure_analysis": {
                "failure_step": failure_step,
                "context_before": context_before,
                "context_after": context_after,
                "potential_causes": await self._identify_potential_causes(
                    context_before, context_after
                ),
                "suggested_fixes": await self._suggest_fixes(
                    trace_id, failure_step
                )
            }
        }
```

#### 4.4 åŸºäºç°æœ‰æ¶æ„çš„å®æ–½è®¡åˆ’

**ç¬¬ä¸€é˜¶æ®µï¼ˆ2-3å‘¨ï¼‰ï¼šå¢å¼ºç°æœ‰TraceWriter**
- [ ] åˆ›å»ºEnhancedTraceWriteræ‰©å±•ç°æœ‰TraceWriterç±»
- [ ] å®ç°TraceAnalyzerå’ŒTraceReplayer
- [ ] é›†æˆåˆ°ç°æœ‰ç›‘æ§å’Œå¥åº·æ£€æŸ¥æœºåˆ¶
- [ ] ä¿æŒç°æœ‰APIå…¼å®¹æ€§

**ç¬¬äºŒé˜¶æ®µï¼ˆ2-3å‘¨ï¼‰ï¼šè°ƒè¯•å·¥å…·å¼€å‘**
- [ ] å®ç°TraceDebuggerå’Œå¯è§†åŒ–å·¥å…·
- [ ] å¼€å‘æ€§èƒ½åˆ†æå’Œå¤±è´¥åˆ†æåŠŸèƒ½
- [ ] é›†æˆåˆ°ç°æœ‰æ—¥å¿—ç³»ç»Ÿ
- [ ] æä¾›Webç•Œé¢å’ŒAPIæ¥å£

**ç¬¬ä¸‰é˜¶æ®µï¼ˆ1-2å‘¨ï¼‰ï¼šä¼ä¸šçº§ç‰¹æ€§**
- [ ] å®ç°AuditTraceWriterå®¡è®¡åŠŸèƒ½
- [ ] æ·»åŠ åˆè§„æ€§æ£€æŸ¥å’ŒæŠ¥å‘Š
- [ ] é›†æˆå‘Šè­¦å’Œé€šçŸ¥æœºåˆ¶
- [ ] æ€§èƒ½ä¼˜åŒ–å’Œæµ‹è¯•

### 5. é•¿æœŸè®°å¿†ä¸åé¦ˆæœºåˆ¶ï¼šæ™ºèƒ½æ¼”åŒ–æ”¯æ’‘ä½“ç³»

#### 5.1 ç°çŠ¶åˆ†æä¸æ”¹è¿›ç›®æ ‡

**ç°æœ‰è®°å¿†èƒ½åŠ›**ï¼š
- âœ… **ç»“æ„åŒ–å­˜å‚¨**ï¼šMemoryEntryæ”¯æŒä¸°å¯Œçš„å…ƒæ•°æ®å’Œåˆ†ç±»
- âœ… **TTLæœºåˆ¶**ï¼šè‡ªåŠ¨è¿‡æœŸæ¸…ç†ï¼Œé¿å…å†…å­˜æ³„æ¼
- âœ… **åˆ†åŒºç®¡ç†**ï¼šé«˜æ•ˆçš„å†…å­˜åˆ†åŒºå’Œç´¢å¼•æœºåˆ¶
- âœ… **è®¿é—®ç»Ÿè®¡**ï¼šaccess_countç­‰ç»Ÿè®¡ä¿¡æ¯

**é•¿æœŸè®°å¿†æŒ‘æˆ˜**ï¼š
- âŒ **è¯­ä¹‰è®°å¿†**ï¼šç¼ºä¹åŸºäºè¯­ä¹‰çš„é•¿æœŸçŸ¥è¯†å­˜å‚¨
- âŒ **ç»éªŒç§¯ç´¯**ï¼šæ— æ³•å½¢æˆè·¨ä»»åŠ¡çš„ç»éªŒæ•°æ®åº“
- âŒ **æ™ºèƒ½æ£€ç´¢**ï¼šæ— æ³•æ ¹æ®è¯­ä¹‰ç›¸ä¼¼æ€§æ£€ç´¢å†å²ç»éªŒ
- âŒ **åé¦ˆé—­ç¯**ï¼šç¼ºä¹ç”¨æˆ·åé¦ˆé©±åŠ¨çš„è¡Œä¸ºä¼˜åŒ–æœºåˆ¶

**æ”¹è¿›ç›®æ ‡**ï¼š
- ğŸ¯ **è¯­ä¹‰è®°å¿†åº“**ï¼šæ„å»ºå¯è¯­ä¹‰æ£€ç´¢çš„é•¿æœŸçŸ¥è¯†åº“
- ğŸ¯ **ç»éªŒæ²‰æ·€**ï¼šè·¨ä»»åŠ¡ç»éªŒç§¯ç´¯å’Œå¤ç”¨
- ğŸ¯ **åé¦ˆå­¦ä¹ **ï¼šåŸºäºåé¦ˆçš„æŒç»­ä¼˜åŒ–æœºåˆ¶
- ğŸ¯ **æ™ºèƒ½æ¨è**ï¼šåŸºäºå†å²ç»éªŒçš„æ™ºèƒ½ç­–ç•¥æ¨è

#### 5.2 åŸºäºç°æœ‰æ¶æ„çš„é•¿æœŸè®°å¿†å¢å¼ºè®¾è®¡

**é›†æˆé•¿æœŸè®°å¿†æ¶æ„**ï¼š
```
ç°æœ‰Memoryç³»ç»Ÿå¢å¼º
â”œâ”€â”€ çŸ­æœŸè®°å¿† (ç°æœ‰memory_enhanced.py)
â”‚   â”œâ”€â”€ MemoryEntry (å·²å®ç°)
â”‚   â”œâ”€â”€ TTLæœºåˆ¶ (å·²å®ç°)
â”‚   â””â”€â”€ åˆ†åŒºç´¢å¼• (å·²å®ç°)
â”œâ”€â”€ é•¿æœŸè®°å¿†å±‚ (æ–°å¢)
â”‚   â”œâ”€â”€ LongTermMemory (æ‰©å±•EnhancedMemory)
â”‚   â”œâ”€â”€ SemanticMemoryStore (è¯­ä¹‰è®°å¿†åº“)
â”‚   â”œâ”€â”€ ExperienceDatabase (ç»éªŒæ•°æ®åº“)
â”‚   â””â”€â”€ MemorySummarizer (è®°å¿†æ‘˜è¦å™¨)
â”œâ”€â”€ åé¦ˆæœºåˆ¶å±‚ (æ–°å¢)
â”‚   â”œâ”€â”€ FeedbackCollector (åé¦ˆæ”¶é›†å™¨)
â”‚   â”œâ”€â”€ FeedbackAnalyzer (åé¦ˆåˆ†æå™¨)
â”‚   â”œâ”€â”€ BehaviorOptimizer (è¡Œä¸ºä¼˜åŒ–å™¨)
â”‚   â””â”€â”€ LearningEngine (å­¦ä¹ å¼•æ“)
â””â”€â”€ æ™ºèƒ½æ¨èå±‚ (æ–°å¢)
    â”œâ”€â”€ ExperienceRetriever (ç»éªŒæ£€ç´¢å™¨)
    â”œâ”€â”€ StrategyRecommender (ç­–ç•¥æ¨èå™¨)
    â”œâ”€â”€ PatternMatcher (æ¨¡å¼åŒ¹é…å™¨)
    â””â”€â”€ AdaptiveReasoner (è‡ªé€‚åº”æ¨ç†å™¨)
```

**æ ¸å¿ƒç»„ä»¶å®ç°**ï¼š

```python
class LongTermMemory(EnhancedMemory):
    """é•¿æœŸè®°å¿†ç®¡ç†å™¨"""
    
    def __init__(self, agent_id: str):
        super().__init__(agent_id, ttl=None)  # é•¿æœŸè®°å¿†ä¸è®¾ç½®TTL
        self.semantic_store = SemanticMemoryStore()
        self.experience_db = ExperienceDatabase()
        self.summarizer = MemorySummarizer()
        
    async def consolidate_memories(self, context_id: str):
        """æ•´åˆçŸ­æœŸè®°å¿†åˆ°é•¿æœŸè®°å¿†"""
        # 1. è·å–ç›¸å…³çš„çŸ­æœŸè®°å¿†
        short_term_memories = await self.get_memories_by_context(context_id)
        
        # 2. ç”Ÿæˆæ‘˜è¦å’Œæå–ç»éªŒ
        consolidated_memory = await self.summarizer.consolidate(
            short_term_memories, context_id
        )
        
        # 3. å­˜å‚¨åˆ°é•¿æœŸè®°å¿†
        long_term_entry = await self.add_memory(
            content=consolidated_memory["summary"],
            memory_type=MemoryType.LONG_TERM,
            context_id=context_id,
            metadata={
                "consolidation_timestamp": datetime.utcnow().isoformat(),
                "source_memories_count": len(short_term_memories),
                "experience_tags": consolidated_memory["experience_tags"],
                "semantic_embedding": consolidated_memory["embedding"]
            }
        )
        
        # 4. å­˜å‚¨åˆ°è¯­ä¹‰è®°å¿†åº“
        await self.semantic_store.add_semantic_memory(
            entry_id=long_term_entry,
            embedding=consolidated_memory["embedding"],
            tags=consolidated_memory["experience_tags"]
        )
        
        return long_term_entry
    
    async def retrieve_relevant_experiences(self, query: str, context: Dict) -> List[Dict]:
        """æ£€ç´¢ç›¸å…³ç»éªŒ"""
        # 1. è¯­ä¹‰æ£€ç´¢
        semantic_results = await self.semantic_store.semantic_search(query)
        
        # 2. æ¨¡å¼åŒ¹é…
        pattern_results = await self.experience_db.find_similar_patterns(context)
        
        # 3. ç»“æœèåˆ
        relevant_experiences = await self._merge_experience_results(
            semantic_results, pattern_results
        )
        
        return relevant_experiences

class SemanticMemoryStore:
    """è¯­ä¹‰è®°å¿†å­˜å‚¨"""
    
    def __init__(self):
        self.embedder = SentenceTransformer("BAAI/bge-large-zh-v1.5")
        self.vector_store = VectorStore()  # å¯ä»¥æ˜¯Chromaã€FAISSç­‰
        
    async def add_semantic_memory(self, entry_id: str, embedding: List[float], 
                                 tags: List[str]):
        """æ·»åŠ è¯­ä¹‰è®°å¿†"""
        await self.vector_store.add(
            id=entry_id,
            vector=embedding,
            metadata={
                "tags": tags,
                "created_at": datetime.utcnow().isoformat()
            }
        )
    
    async def semantic_search(self, query: str, top_k: int = 5) -> List[Dict]:
        """è¯­ä¹‰æ£€ç´¢"""
        query_embedding = self.embedder.encode([query])[0].tolist()
        
        results = await self.vector_store.query(
            vector=query_embedding,
            top_k=top_k
        )
        
        return results

class ExperienceDatabase:
    """ç»éªŒæ•°æ®åº“"""
    
    def __init__(self):
        self.experiences = {}  # task_pattern -> experience_data
        self.pattern_matcher = PatternMatcher()
        
    async def record_experience(self, task_context: Dict, execution_result: Dict, 
                               feedback: Optional[Dict] = None):
        """è®°å½•æ‰§è¡Œç»éªŒ"""
        # 1. æå–ä»»åŠ¡æ¨¡å¼
        task_pattern = await self.pattern_matcher.extract_pattern(task_context)
        
        # 2. æ„å»ºç»éªŒè®°å½•
        experience = {
            "pattern": task_pattern,
            "context": task_context,
            "execution": execution_result,
            "feedback": feedback,
            "success_rate": self._calculate_success_rate(task_pattern),
            "optimization_suggestions": await self._generate_optimizations(
                task_context, execution_result, feedback
            ),
            "recorded_at": datetime.utcnow().isoformat()
        }
        
        # 3. å­˜å‚¨ç»éªŒ
        pattern_key = self._generate_pattern_key(task_pattern)
        self.experiences.setdefault(pattern_key, []).append(experience)
        
        return experience
    
    async def find_similar_patterns(self, current_context: Dict) -> List[Dict]:
        """æŸ¥æ‰¾ç›¸ä¼¼æ¨¡å¼çš„ç»éªŒ"""
        current_pattern = await self.pattern_matcher.extract_pattern(current_context)
        
        similar_experiences = []
        for pattern_key, experiences in self.experiences.items():
            similarity = await self.pattern_matcher.calculate_similarity(
                current_pattern, experiences[0]["pattern"]
            )
            
            if similarity > 0.7:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                similar_experiences.extend(experiences)
        
        # æŒ‰æˆåŠŸç‡å’Œç›¸ä¼¼åº¦æ’åº
        similar_experiences.sort(
            key=lambda x: (x["success_rate"], similarity), 
            reverse=True
        )
        
        return similar_experiences[:5]

class FeedbackCollector:
    """åé¦ˆæ”¶é›†å™¨"""
    
    def __init__(self):
        self.feedback_store = {}
        self.feedback_analyzer = FeedbackAnalyzer()
        
    async def collect_user_feedback(self, trace_id: str, feedback_data: Dict):
        """æ”¶é›†ç”¨æˆ·åé¦ˆ"""
        feedback_entry = {
            "trace_id": trace_id,
            "feedback_type": "user",
            "rating": feedback_data.get("rating"),
            "comments": feedback_data.get("comments"),
            "specific_issues": feedback_data.get("issues", []),
            "suggestions": feedback_data.get("suggestions", []),
            "timestamp": datetime.utcnow().isoformat()
        }
        
        self.feedback_store.setdefault(trace_id, []).append(feedback_entry)
        
        # å®æ—¶åˆ†æåé¦ˆ
        analysis = await self.feedback_analyzer.analyze_feedback(feedback_entry)
        
        return {
            "feedback_id": f"{trace_id}-{len(self.feedback_store[trace_id])}",
            "analysis": analysis
        }
    
    async def collect_system_feedback(self, trace_id: str, performance_metrics: Dict):
        """æ”¶é›†ç³»ç»Ÿåé¦ˆ"""
        system_feedback = {
            "trace_id": trace_id,
            "feedback_type": "system",
            "performance_metrics": performance_metrics,
            "anomalies": await self._detect_performance_anomalies(performance_metrics),
            "optimization_opportunities": await self._identify_optimizations(performance_metrics),
            "timestamp": datetime.utcnow().isoformat()
        }
        
        self.feedback_store.setdefault(trace_id, []).append(system_feedback)
        
        return system_feedback

class BehaviorOptimizer:
    """è¡Œä¸ºä¼˜åŒ–å™¨ - åŸºäºåé¦ˆä¼˜åŒ–Agentè¡Œä¸º"""
    
    def __init__(self):
        self.optimization_rules = {}
        self.learning_engine = LearningEngine()
        
    async def optimize_based_on_feedback(self, agent_id: str, 
                                       feedback_data: List[Dict]) -> Dict:
        """åŸºäºåé¦ˆä¼˜åŒ–è¡Œä¸º"""
        # 1. åˆ†æåé¦ˆæ¨¡å¼
        feedback_patterns = await self._analyze_feedback_patterns(feedback_data)
        
        # 2. ç”Ÿæˆä¼˜åŒ–ç­–ç•¥
        optimization_strategies = await self._generate_optimization_strategies(
            agent_id, feedback_patterns
        )
        
        # 3. åº”ç”¨ä¼˜åŒ–
        optimization_results = []
        for strategy in optimization_strategies:
            result = await self._apply_optimization_strategy(agent_id, strategy)
            optimization_results.append(result)
        
        return {
            "agent_id": agent_id,
            "feedback_patterns": feedback_patterns,
            "applied_optimizations": optimization_results,
            "expected_improvements": await self._predict_improvements(
                optimization_strategies
            )
        }
    
    async def _generate_optimization_strategies(self, agent_id: str, 
                                              patterns: Dict) -> List[Dict]:
        """ç”Ÿæˆä¼˜åŒ–ç­–ç•¥"""
        strategies = []
        
        # åŸºäºåé¦ˆæ¨¡å¼ç”Ÿæˆä¸åŒçš„ä¼˜åŒ–ç­–ç•¥
        if patterns.get("response_too_long"):
            strategies.append({
                "type": "response_length_optimization",
                "action": "adjust_summary_parameters",
                "parameters": {"max_length": patterns["preferred_length"]}
            })
        
        if patterns.get("accuracy_issues"):
            strategies.append({
                "type": "accuracy_improvement",
                "action": "enhance_verification_steps",
                "parameters": {"verification_threshold": 0.9}
            })
        
        if patterns.get("speed_concerns"):
            strategies.append({
                "type": "performance_optimization",
                "action": "optimize_tool_selection",
                "parameters": {"prefer_fast_tools": True}
            })
        
        return strategies
```

#### 5.3 æ™ºèƒ½æ¼”åŒ–é—­ç¯è®¾è®¡

**åé¦ˆé©±åŠ¨çš„å­¦ä¹ é—­ç¯**ï¼š
```python
class LearningEngine:
    """å­¦ä¹ å¼•æ“ - å®ç°åé¦ˆé©±åŠ¨çš„æŒç»­ä¼˜åŒ–"""
    
    def __init__(self):
        self.learning_algorithms = {
            "reinforcement": ReinforcementLearner(),
            "preference": PreferenceLearner(),
            "pattern": PatternLearner()
        }
        
    async def continuous_learning_cycle(self, agent_id: str):
        """æŒç»­å­¦ä¹ å¾ªç¯"""
        while True:
            # 1. æ”¶é›†æœ€æ–°åé¦ˆ
            recent_feedback = await self._collect_recent_feedback(agent_id)
            
            # 2. åˆ†æå­¦ä¹ æœºä¼š
            learning_opportunities = await self._identify_learning_opportunities(
                recent_feedback
            )
            
            # 3. åº”ç”¨å­¦ä¹ ç®—æ³•
            for opportunity in learning_opportunities:
                learner = self.learning_algorithms[opportunity["type"]]
                learning_result = await learner.learn(opportunity["data"])
                
                # 4. æ›´æ–°è¡Œä¸ºç­–ç•¥
                await self._update_behavior_strategy(agent_id, learning_result)
            
            # 5. éªŒè¯æ”¹è¿›æ•ˆæœ
            improvement_metrics = await self._measure_improvements(agent_id)
            
            # 6. è®°å½•å­¦ä¹ å†å²
            await self._record_learning_history(agent_id, {
                "learning_opportunities": learning_opportunities,
                "improvements": improvement_metrics,
                "timestamp": datetime.utcnow().isoformat()
            })
            
            # ç­‰å¾…ä¸‹ä¸€ä¸ªå­¦ä¹ å‘¨æœŸ
            await asyncio.sleep(3600)  # æ¯å°æ—¶å­¦ä¹ ä¸€æ¬¡
```

#### 5.4 åŸºäºç°æœ‰æ¶æ„çš„å®æ–½è®¡åˆ’

**ç¬¬ä¸€é˜¶æ®µï¼ˆ3-4å‘¨ï¼‰ï¼šé•¿æœŸè®°å¿†åŸºç¡€**
- [ ] åˆ›å»ºLongTermMemoryæ‰©å±•ç°æœ‰EnhancedMemoryç±»
- [ ] å®ç°SemanticMemoryStoreå’ŒExperienceDatabase
- [ ] é›†æˆå‘é‡æ•°æ®åº“ï¼ˆChromaæ¨èï¼‰
- [ ] å®ç°è®°å¿†æ•´åˆå’Œè¯­ä¹‰æ£€ç´¢åŠŸèƒ½

**ç¬¬äºŒé˜¶æ®µï¼ˆ2-3å‘¨ï¼‰ï¼šåé¦ˆæœºåˆ¶**
- [ ] å®ç°FeedbackCollectorå’ŒFeedbackAnalyzer
- [ ] å¼€å‘BehaviorOptimizerè¡Œä¸ºä¼˜åŒ–å™¨
- [ ] é›†æˆåˆ°ç°æœ‰TraceWriterå’Œç›‘æ§ä½“ç³»
- [ ] å®ç°ç”¨æˆ·åé¦ˆæ”¶é›†æ¥å£

**ç¬¬ä¸‰é˜¶æ®µï¼ˆ2-3å‘¨ï¼‰ï¼šå­¦ä¹ å¼•æ“**
- [ ] å®ç°LearningEngineæŒç»­å­¦ä¹ æœºåˆ¶
- [ ] å¼€å‘å¤šç§å­¦ä¹ ç®—æ³•ï¼ˆå¼ºåŒ–å­¦ä¹ ã€åå¥½å­¦ä¹ ç­‰ï¼‰
- [ ] é›†æˆæ™ºèƒ½æ¨èå’Œè‡ªé€‚åº”æ¨ç†
- [ ] æ€§èƒ½ä¼˜åŒ–å’Œæ•ˆæœè¯„ä¼°

### 6. å®æ—¶åŒæ­¥ï¼šå¤šAgenté—´ä¸Šä¸‹æ–‡å…±äº«

#### 6.1 é—®é¢˜åˆ†æ

**å¤šAgentåä½œæŒ‘æˆ˜**ï¼š
- Agenté—´ä¸Šä¸‹æ–‡éš”ç¦»
- å®æ—¶çŠ¶æ€åŒæ­¥éœ€æ±‚
- æƒé™å’Œå®‰å…¨æ§åˆ¶
- åä½œæ•ˆç‡ä¼˜åŒ–

**å®æ—¶åŒæ­¥ç›®æ ‡**ï¼š
- æ¯«ç§’çº§ä¸Šä¸‹æ–‡åŒæ­¥
- ç»†ç²’åº¦æƒé™æ§åˆ¶
- å†²çªè‡ªåŠ¨è§£å†³
- åä½œæ¨¡å¼æ”¯æŒ

#### 4.2 æŠ€æœ¯æ–¹æ¡ˆè®¾è®¡

**å®æ—¶åŒæ­¥æ¶æ„**ï¼š
```
å¤šAgentä¸Šä¸‹æ–‡å…±äº«ç³»ç»Ÿ
â”œâ”€â”€ å®æ—¶é€šä¿¡å±‚ (RealtimeCommunication)
â”‚   â”œâ”€â”€ WebSocketæœåŠ¡å™¨ (WebSocketServer)
â”‚   â”œâ”€â”€ æ¶ˆæ¯é˜Ÿåˆ— (MessageQueue)
â”‚   â””â”€â”€ äº‹ä»¶æ€»çº¿ (EventBus)
â”œâ”€â”€ åŒæ­¥åè°ƒå±‚ (SynchronizationCoordination)
â”‚   â”œâ”€â”€ çŠ¶æ€åŒæ­¥å™¨ (StateSynchronizer)
â”‚   â”œâ”€â”€ å†²çªè§£å†³å™¨ (ConflictResolver)
â”‚   â””â”€â”€ ç‰ˆæœ¬æ§åˆ¶å™¨ (VersionController)
â”œâ”€â”€ æƒé™æ§åˆ¶å±‚ (PermissionControl)
â”‚   â”œâ”€â”€ è®¿é—®æ§åˆ¶ (AccessControl)
â”‚   â”œâ”€â”€ å…±äº«ç­–ç•¥ (SharingPolicy)
â”‚   â””â”€â”€ å®‰å…¨å®¡è®¡ (SecurityAudit)
â””â”€â”€ åä½œç®¡ç†å±‚ (CollaborationManagement)
    â”œâ”€â”€ åä½œæ¨¡å¼ (CollaborationMode)
    â”œâ”€â”€ å·¥ä½œæµåè°ƒ (WorkflowCoordination)
    â””â”€â”€ æ€§èƒ½ä¼˜åŒ– (PerformanceOptimization)
```

**æ ¸å¿ƒå®ç°**ï¼š

```python
class MultiAgentContextSharing:
    """å¤šAgentä¸Šä¸‹æ–‡å…±äº«ç³»ç»Ÿ"""
    
    def __init__(self):
        self.realtime_communicator = RealtimeCommunicator()
        self.state_synchronizer = StateSynchronizer()
        self.permission_manager = PermissionManager()
        self.collaboration_manager = CollaborationManager()
        
    async def create_shared_context(
        self, 
        context: Context, 
        sharing_policy: SharingPolicy
    ) -> SharedContext:
        """åˆ›å»ºå…±äº«ä¸Šä¸‹æ–‡"""
        # 1. åˆ›å»ºå…±äº«ä¸Šä¸‹æ–‡å¯¹è±¡
        shared_context = SharedContext(
            original_context=context,
            sharing_policy=sharing_policy,
            created_by=context.agent_id
        )
        
        # 2. è®¾ç½®æƒé™ç­–ç•¥
        await self.permission_manager.setup_permissions(
            shared_context, sharing_policy
        )
        
        # 3. åˆå§‹åŒ–åŒæ­¥æœºåˆ¶
        await self.state_synchronizer.initialize_sync(shared_context)
        
        # 4. é€šçŸ¥ç›¸å…³Agent
        await self._notify_agents(shared_context, "context_shared")
        
        return shared_context
    
    async def join_shared_context(
        self, 
        agent_id: str, 
        shared_context_id: str
    ) -> bool:
        """åŠ å…¥å…±äº«ä¸Šä¸‹æ–‡"""
        # 1. æƒé™æ£€æŸ¥
        has_permission = await self.permission_manager.check_join_permission(
            agent_id, shared_context_id
        )
        
        if not has_permission:
            return False
        
        # 2. è·å–å…±äº«ä¸Šä¸‹æ–‡
        shared_context = await self._get_shared_context(shared_context_id)
        
        # 3. åŒæ­¥å½“å‰çŠ¶æ€
        await self.state_synchronizer.sync_agent_state(
            agent_id, shared_context
        )
        
        # 4. å»ºç«‹å®æ—¶è¿æ¥
        await self.realtime_communicator.establish_connection(
            agent_id, shared_context_id
        )
        
        # 5. é€šçŸ¥å…¶ä»–å‚ä¸è€…
        await self._notify_participants(shared_context, agent_id, "agent_joined")
        
        return True
    
    async def update_shared_context(
        self, 
        agent_id: str, 
        shared_context_id: str, 
        update: ContextUpdate
    ) -> None:
        """æ›´æ–°å…±äº«ä¸Šä¸‹æ–‡"""
        # 1. æƒé™éªŒè¯
        await self._verify_update_permission(agent_id, shared_context_id, update)
        
        # 2. å†²çªæ£€æµ‹
        conflicts = await self.state_synchronizer.detect_conflicts(
            shared_context_id, update
        )
        
        # 3. å†²çªè§£å†³
        if conflicts:
            resolved_update = await self._resolve_update_conflicts(
                conflicts, update
            )
        else:
            resolved_update = update
        
        # 4. åº”ç”¨æ›´æ–°
        await self._apply_context_update(shared_context_id, resolved_update)
        
        # 5. å®æ—¶å¹¿æ’­
        await self.realtime_communicator.broadcast_update(
            shared_context_id, resolved_update, exclude_agent=agent_id
        )

class StateSynchronizer:
    """çŠ¶æ€åŒæ­¥å™¨"""
    
    def __init__(self):
        self.version_controller = VersionController()
        self.conflict_resolver = ConflictResolver()
        self.sync_strategies = {
            "immediate": ImmediateSyncStrategy(),
            "batched": BatchedSyncStrategy(),
            "optimistic": OptimisticSyncStrategy()
        }
    
    async def sync_agent_state(
        self, 
        agent_id: str, 
        shared_context: SharedContext
    ) -> None:
        """åŒæ­¥AgentçŠ¶æ€"""
        # 1. è·å–æœ€æ–°ç‰ˆæœ¬
        latest_version = await self.version_controller.get_latest_version(
            shared_context.context_id
        )
        
        # 2. è®¡ç®—å·®å¼‚
        state_diff = await self._calculate_state_diff(
            agent_id, shared_context, latest_version
        )
        
        # 3. åº”ç”¨å·®å¼‚
        if state_diff:
            await self._apply_state_diff(agent_id, state_diff)
        
        # 4. æ›´æ–°ç‰ˆæœ¬æ ‡è®°
        await self.version_controller.update_agent_version(
            agent_id, shared_context.context_id, latest_version
        )
    
    async def detect_conflicts(
        self, 
        shared_context_id: str, 
        update: ContextUpdate
    ) -> List[Conflict]:
        """æ£€æµ‹æ›´æ–°å†²çª"""
        conflicts = []
        
        # 1. è·å–å¹¶å‘æ›´æ–°
        concurrent_updates = await self._get_concurrent_updates(
            shared_context_id, update.timestamp
        )
        
        # 2. æ£€æµ‹å†²çªç±»å‹
        for concurrent_update in concurrent_updates:
            conflict_type = self._analyze_conflict_type(update, concurrent_update)
            
            if conflict_type != ConflictType.NO_CONFLICT:
                conflicts.append(Conflict(
                    type=conflict_type,
                    update1=update,
                    update2=concurrent_update,
                    resolution_strategy=self._select_resolution_strategy(conflict_type)
                ))
        
        return conflicts

class CollaborationManager:
    """åä½œç®¡ç†å™¨"""
    
    def __init__(self):
        self.collaboration_modes = {
            "sequential": SequentialCollaboration(),
            "parallel": ParallelCollaboration(),
            "hierarchical": HierarchicalCollaboration(),
            "democratic": DemocraticCollaboration()
        }
    
    async def setup_collaboration_mode(
        self, 
        shared_context: SharedContext, 
        mode: str
    ) -> None:
        """è®¾ç½®åä½œæ¨¡å¼"""
        collaboration_handler = self.collaboration_modes.get(mode)
        
        if collaboration_handler:
            await collaboration_handler.setup(shared_context)
            shared_context.collaboration_mode = mode
        else:
            raise ValueError(f"Unsupported collaboration mode: {mode}")
    
    async def coordinate_agent_actions(
        self, 
        shared_context_id: str, 
        agent_actions: List[AgentAction]
    ) -> List[CoordinatedAction]:
        """åè°ƒAgentè¡Œä¸º"""
        shared_context = await self._get_shared_context(shared_context_id)
        collaboration_handler = self.collaboration_modes[
            shared_context.collaboration_mode
        ]
        
        return await collaboration_handler.coordinate(agent_actions)
```

**åä½œæ¨¡å¼**ï¼š
1. **é¡ºåºåä½œ**ï¼šAgentæŒ‰é¡ºåºå¤„ç†å…±äº«ä¸Šä¸‹æ–‡
2. **å¹¶è¡Œåä½œ**ï¼šå¤šAgentåŒæ—¶å¤„ç†ä¸åŒéƒ¨åˆ†
3. **å±‚æ¬¡åä½œ**ï¼šä¸»ä»Agentåä½œæ¨¡å¼
4. **æ°‘ä¸»åä½œ**ï¼šæŠ•ç¥¨å†³ç­–æœºåˆ¶

#### 4.3 å®æ–½è®¡åˆ’

**ç¬¬ä¸€é˜¶æ®µï¼ˆ3-4å‘¨ï¼‰ï¼šå®æ—¶é€šä¿¡åŸºç¡€**
- [ ] å®ç°WebSocketæœåŠ¡å™¨å’Œæ¶ˆæ¯é˜Ÿåˆ—
- [ ] åŸºç¡€çŠ¶æ€åŒæ­¥æœºåˆ¶
- [ ] æƒé™æ§åˆ¶ç³»ç»Ÿ
- [ ] ç®€å•åä½œæ¨¡å¼

**ç¬¬äºŒé˜¶æ®µï¼ˆ3-4å‘¨ï¼‰ï¼šé«˜çº§åä½œåŠŸèƒ½**
- [ ] å†²çªæ£€æµ‹å’Œè§£å†³
- [ ] ç‰ˆæœ¬æ§åˆ¶ç³»ç»Ÿ
- [ ] å¤šç§åä½œæ¨¡å¼
- [ ] æ€§èƒ½ä¼˜åŒ–

**ç¬¬ä¸‰é˜¶æ®µï¼ˆ2-3å‘¨ï¼‰ï¼šç›‘æ§å’Œä¼˜åŒ–**
- [ ] åä½œæ•ˆæœç›‘æ§
- [ ] æ€§èƒ½è°ƒä¼˜
- [ ] å®‰å…¨å®¡è®¡
- [ ] ç”¨æˆ·ç•Œé¢

## åŸºäºç°æœ‰æ¶æ„çš„æŠ€æœ¯æŒ‡æ ‡å’Œé¢„æœŸæ•ˆæœ

### æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”

| æ”¹è¿›æ–¹å‘ | ç°æœ‰èƒ½åŠ› | å¢å¼ºåæ€§èƒ½ | é›†æˆæ–¹å¼ |
|---------|---------|---------|---------|
| **å‘é‡åŒ–æ£€ç´¢** | ç²¾ç¡®åŒ¹é…(context_id/trace_id) | è¯­ä¹‰æ£€ç´¢90%å‡†ç¡®ç‡ | æ‰©å±•ç°æœ‰Contextå’ŒMemoryç±» |
| **æ™ºèƒ½å‹ç¼©** | ç®€å•æˆªæ–­(max_history/TTL) | 70%å‹ç¼©ç‡ï¼Œ90%ä¿¡æ¯ä¿ç•™ | å¢å¼ºç°æœ‰å‹ç¼©æœºåˆ¶ |
| **åˆ†å¸ƒå¼å­˜å‚¨** | å•èŠ‚ç‚¹å­˜å‚¨+å¿«ç…§ | 99.9%å¯ç”¨æ€§ï¼Œ<50mså»¶è¿Ÿ | æ‰©å±•ç°æœ‰å¿«ç…§å’Œåˆ†åŒºæœºåˆ¶ |
| **å¤šAgentåä½œ** | å•Agentä¸Šä¸‹æ–‡ | <100msåŒæ­¥å»¶è¿Ÿ | åŸºäºç°æœ‰trace_idå®ç°åä½œ |

### ç³»ç»Ÿå®¹é‡æå‡

- **æ£€ç´¢èƒ½åŠ›**ï¼šä»ç²¾ç¡®åŒ¹é…æ‰©å±•åˆ°è¯­ä¹‰æ£€ç´¢ï¼Œä¿æŒç°æœ‰O(1)æ€§èƒ½
- **å­˜å‚¨æ•ˆç‡**ï¼šåœ¨ç°æœ‰TTLåŸºç¡€ä¸Šå¢åŠ æ™ºèƒ½å‹ç¼©ï¼Œæå‡70%å­˜å‚¨æ•ˆç‡
- **å¯ç”¨æ€§**ï¼šä»å•ç‚¹æ‰©å±•åˆ°åˆ†å¸ƒå¼ï¼Œä¿æŒç°æœ‰APIå…¼å®¹æ€§
- **åä½œèƒ½åŠ›**ï¼šåŸºäºç°æœ‰Sessionæœºåˆ¶å®ç°å¤šAgentåä½œ

### ä¸šåŠ¡ä»·å€¼

1. **æ¸è¿›å¼å‡çº§**ï¼šåŸºäºç°æœ‰æ¶æ„å¢å¼ºï¼Œæ— éœ€é‡æ„
2. **å‘åå…¼å®¹**ï¼šä¿æŒç°æœ‰APIå’Œæ•°æ®ç»“æ„ä¸å˜
3. **æ€§èƒ½æå‡**ï¼šåœ¨ç°æœ‰åŸºç¡€ä¸Šæ˜¾è‘—æå‡æ™ºèƒ½åŒ–æ°´å¹³
4. **é£é™©å¯æ§**ï¼šåˆ©ç”¨ç°æœ‰ç›‘æ§å’Œå¥åº·æ£€æŸ¥æœºåˆ¶

## åŸºäºç°æœ‰æ¶æ„çš„å®æ–½è·¯çº¿å›¾

### æ€»ä½“æ—¶é—´è§„åˆ’ï¼ˆ18-22å‘¨ï¼‰

```
Phase 1: å‘é‡åŒ–æ£€ç´¢å¢å¼º (Week 1-5)
â”œâ”€â”€ æ‰©å±•ç°æœ‰Contextå’ŒMemoryç±» (Week 1-2)
â”œâ”€â”€ é›†æˆå‘é‡åŒ–æ£€ç´¢ (Week 3-4)
â””â”€â”€ ä¸ç°æœ‰ç›‘æ§é›†æˆ (Week 5)

Phase 2: æ™ºèƒ½å‹ç¼©å¢å¼º (Week 6-10)
â”œâ”€â”€ å¢å¼ºç°æœ‰max_historyå’ŒTTLæœºåˆ¶ (Week 6-7)
â”œâ”€â”€ å®ç°æ™ºèƒ½é‡è¦æ€§è¯„ä¼° (Week 8-9)
â””â”€â”€ é›†æˆå‹ç¼©ç®—æ³• (Week 10)

Phase 3: TraceWriterå¢å¼º (Week 11-15)
â”œâ”€â”€ å¢å¼ºç°æœ‰TraceWriter (Week 11-12)
â”œâ”€â”€ è°ƒè¯•å·¥å…·å¼€å‘ (Week 13-14)
â””â”€â”€ ä¼ä¸šçº§ç‰¹æ€§ (Week 15)

Phase 4: é•¿æœŸè®°å¿†ä¸åé¦ˆæœºåˆ¶ (Week 16-22)
â”œâ”€â”€ é•¿æœŸè®°å¿†åŸºç¡€ (Week 16-19)
â”œâ”€â”€ åé¦ˆæœºåˆ¶ (Week 20-21)
â””â”€â”€ å­¦ä¹ å¼•æ“ (Week 22)

Phase 5: åˆ†å¸ƒå¼å­˜å‚¨å¢å¼º (Week 23-26)
â”œâ”€â”€ æ‰©å±•ç°æœ‰å¿«ç…§å’Œåˆ†åŒºæœºåˆ¶ (Week 23-24)
â”œâ”€â”€ å®ç°è·¨èŠ‚ç‚¹å¤åˆ¶ (Week 25)
â””â”€â”€ é›†æˆä¸€è‡´æ€§ç®¡ç† (Week 26)

Phase 6: å¤šAgentåä½œå¢å¼º (Week 27-28)
â”œâ”€â”€ åŸºäºç°æœ‰trace_idå®ç°åä½œ (Week 27)
â””â”€â”€ ç³»ç»Ÿæ•´åˆå’Œä¼˜åŒ–æµ‹è¯• (Week 28)
```

### ä¼˜å…ˆçº§æ’åºï¼ˆåŸºäºç°æœ‰æ¶æ„æˆç†Ÿåº¦å’Œä¸šåŠ¡ä»·å€¼ï¼‰

1. **é«˜ä¼˜å…ˆçº§**ï¼šå‘é‡åŒ–æ£€ç´¢ï¼ˆæ‰©å±•ç°æœ‰æ£€ç´¢èƒ½åŠ›ï¼Œæå‡ç”¨æˆ·ä½“éªŒï¼‰
2. **é«˜ä¼˜å…ˆçº§**ï¼šæ™ºèƒ½å‹ç¼©ï¼ˆå¢å¼ºç°æœ‰å‹ç¼©æœºåˆ¶ï¼Œé™ä½å­˜å‚¨æˆæœ¬ï¼‰
3. **é«˜ä¼˜å…ˆçº§**ï¼šTraceWriterå¢å¼ºï¼ˆä¼ä¸šçº§è°ƒè¯•å’Œå®¡è®¡éœ€æ±‚ï¼‰
4. **ä¸­ä¼˜å…ˆçº§**ï¼šé•¿æœŸè®°å¿†ä¸åé¦ˆæœºåˆ¶ï¼ˆæ™ºèƒ½æ¼”åŒ–èƒ½åŠ›ï¼‰
5. **ä¸­ä¼˜å…ˆçº§**ï¼šåˆ†å¸ƒå¼å­˜å‚¨ï¼ˆæ‰©å±•ç°æœ‰å¿«ç…§æœºåˆ¶ï¼Œæå‡å¯é æ€§ï¼‰
6. **ä½ä¼˜å…ˆçº§**ï¼šå¤šAgentåä½œï¼ˆåŸºäºç°æœ‰Sessionå®ç°ï¼Œå¢å¼ºåä½œï¼‰

### é£é™©æ§åˆ¶ç­–ç•¥

- **å…¼å®¹æ€§é£é™©**ï¼šæ‰€æœ‰å¢å¼ºéƒ½åŸºäºæ‰©å±•ç°æœ‰ç±»ï¼Œä¿æŒAPIå…¼å®¹
- **æ€§èƒ½é£é™©**ï¼šåˆ©ç”¨ç°æœ‰åˆ†åŒºç´¢å¼•å’Œå¥åº·æ£€æŸ¥æœºåˆ¶
- **æ•°æ®é£é™©**ï¼šåŸºäºç°æœ‰å¿«ç…§å’ŒTTLæœºåˆ¶ï¼Œæ•°æ®å®‰å…¨å¯æ§
- **è¿ç»´é£é™©**ï¼šå¤ç”¨ç°æœ‰ç›‘æ§ä½“ç³»ï¼Œé™ä½è¿ç»´å¤æ‚åº¦

## æ€»ç»“

æœ¬æ”¹è¿›æ–¹æ¡ˆå®Œå…¨åŸºäºæ‚¨ç°æœ‰ä¼ä¸šçº§Agentç³»ç»Ÿçš„æˆç†Ÿæ¶æ„ï¼Œé€šè¿‡**æ‰©å±•è€Œéé‡æ„**çš„æ–¹å¼ï¼Œå®ç°å››å¤§æ ¸å¿ƒèƒ½åŠ›çš„ç³»ç»Ÿæ€§æå‡ã€‚æ–¹æ¡ˆæœ€å¤§çš„ä¼˜åŠ¿æ˜¯**å®Œå…¨å…¼å®¹ç°æœ‰ä»£ç **ï¼Œæ‰€æœ‰æ”¹è¿›éƒ½é€šè¿‡ç»§æ‰¿å’Œæ‰©å±•ç°æœ‰ç±»æ¥å®ç°ã€‚

### æ ¸å¿ƒæ”¹è¿›ç­–ç•¥

1. **VectorizedContext/Memory** - æ‰©å±•ç°æœ‰Contextå’ŒEnhancedMemoryç±»ï¼Œå¢åŠ è¯­ä¹‰æ£€ç´¢èƒ½åŠ›
2. **IntelligentContext/CompressedMemory** - å¢å¼ºç°æœ‰max_historyå’ŒTTLæœºåˆ¶ï¼Œå®ç°æ™ºèƒ½å‹ç¼©
3. **EnhancedTraceWriter** - æ‰©å±•ç°æœ‰TraceWriterï¼Œå¢åŠ ä¼ä¸šçº§è°ƒè¯•ã€å›æ”¾ã€å®¡è®¡èƒ½åŠ›
4. **LongTermMemory + FeedbackEngine** - æ„å»ºè¯­ä¹‰è®°å¿†åº“å’Œåé¦ˆé©±åŠ¨çš„å­¦ä¹ æœºåˆ¶
5. **DistributedContext/Memory/Session** - æ‰©å±•ç°æœ‰å¿«ç…§å’Œåˆ†åŒºæœºåˆ¶ï¼Œå®ç°åˆ†å¸ƒå¼å­˜å‚¨
6. **åŸºäºtrace_idçš„åä½œ** - åˆ©ç”¨ç°æœ‰trace_idå®ç°å¤šAgentåä½œ

### å®æ–½ä¼˜åŠ¿

- âœ… **é›¶é£é™©å‡çº§**ï¼šåŸºäºæ‰©å±•ç°æœ‰ç±»ï¼Œä¿æŒ100%å‘åå…¼å®¹
- âœ… **æ¸è¿›å¼éƒ¨ç½²**ï¼šå¯ä»¥é€æ­¥æ›¿æ¢ç°æœ‰ç»„ä»¶ï¼Œé£é™©å¯æ§
- âœ… **å¤ç”¨ç°æœ‰ä¼˜åŠ¿**ï¼šå……åˆ†åˆ©ç”¨ç°æœ‰åˆ†åŒºç´¢å¼•ã€å¥åº·æ£€æŸ¥ã€ç›‘æ§ä½“ç³»
- âœ… **å¼€å‘æ•ˆç‡é«˜**ï¼šåŸºäºæˆç†Ÿæ¶æ„ï¼Œç›¸æ¯”ä»é›¶æ„å»ºèŠ‚çœ60%å¼€å‘æ—¶é—´

### é¢„æœŸæ•ˆæœ

é€šè¿‡è¿™äº›åŸºäºç°æœ‰æ¶æ„çš„æ”¹è¿›ï¼Œæ‚¨çš„ä¼ä¸šçº§Agentç³»ç»Ÿå°†åœ¨ä¿æŒåŸæœ‰ç¨³å®šæ€§çš„åŸºç¡€ä¸Šè·å¾—ï¼š
- **æ™ºèƒ½åŒ–å¢å¼º**ï¼šè¯­ä¹‰æ£€ç´¢ã€æ™ºèƒ½å‹ç¼©ã€é•¿æœŸè®°å¿†ã€åé¦ˆå­¦ä¹ èƒ½åŠ›
- **ä¼ä¸šçº§èƒ½åŠ›**ï¼šå®Œæ•´çš„è°ƒè¯•ã€å›æ”¾ã€å®¡è®¡ã€åˆè§„æ”¯æŒ
- **å¯é æ€§æå‡**ï¼šåˆ†å¸ƒå¼æ¶æ„å’Œé«˜å¯ç”¨ä¿éšœ  
- **æ€§èƒ½ä¼˜åŒ–**ï¼šåœ¨ç°æœ‰é«˜æ€§èƒ½åŸºç¡€ä¸Šè¿›ä¸€æ­¥æå‡
- **åä½œèƒ½åŠ›**ï¼šå¤šAgentåä½œå’Œå®æ—¶åŒæ­¥
- **æ¼”åŒ–èƒ½åŠ›**ï¼šåŸºäºåé¦ˆçš„æŒç»­å­¦ä¹ å’Œè‡ªæˆ‘ä¼˜åŒ–

è¿™ç§åŸºäºç°æœ‰æ¶æ„çš„æ¸è¿›å¼æ”¹è¿›ç­–ç•¥ï¼Œå°†ä¸ºæ‚¨çš„ç³»ç»Ÿåœ¨å¤§è§„æ¨¡ä¼ä¸šç¯å¢ƒä¸­çš„ç¨³å®šè¿è¡Œå’ŒæŒç»­æ¼”è¿›æä¾›æœ€ä½³è·¯å¾„ã€‚